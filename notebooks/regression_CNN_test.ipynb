{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "regression_CNN_test.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOxyKGwuPX0cTpt7zRg+d7v",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/masdesouza/FDIA-PdM/blob/master/notebooks/regression_CNN_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1I8rNysMF44"
      },
      "source": [
        "# 1. Import modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yU_xGn2HiAsS"
      },
      "source": [
        "import tensorflow.keras\n",
        "from tensorflow.keras.layers import Conv1D, BatchNormalization, Dropout, Dense, InputLayer, Flatten, MaxPool1D, Activation, GlobalAveragePooling1D\n",
        "from tensorflow.keras.activations import relu, softmax\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.losses import binary_crossentropy\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.models import Sequential,load_model\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from sklearn import preprocessing\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from sklearn import preprocessing"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0vMIy33MmXN"
      },
      "source": [
        "# 2. Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhKJNUhIMFlG"
      },
      "source": [
        "# Setting seed for reproducibility\n",
        "np.random.seed(1234)  \n",
        "PYTHONHASHSEED = 0\n",
        "\n",
        "# define path to save model\n",
        "model_path = '../../Output/regression_model_CNN.h5'"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmR5hBQOMvqw"
      },
      "source": [
        "## 2.1 Import dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8Ow2PK3Myyg",
        "outputId": "24659aa9-a0d2-43cb-daf8-c822fbe7a192",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import os\n",
        "import pickle\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "os.chdir(\"/content/drive/MyDrive\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9Lb0NKBM-Yp"
      },
      "source": [
        "# 3. Data Ingestion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSDTjpEBM_-e"
      },
      "source": [
        "# read training data - specify the path for the train data\n",
        "train_df = pd.read_csv('/content/drive/MyDrive/PFC/FDIA-PdM/Datasets/Training/PM_train.txt', sep=\" \", header=None)\n",
        "train_df.drop(train_df.columns[[26, 27]], axis=1, inplace=True)\n",
        "train_df.columns = ['id', 'cycle', 'setting1', 'setting2', 'setting3', 's1', 's2', 's3',\n",
        "                     's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14',\n",
        "                     's15', 's16', 's17', 's18', 's19', 's20', 's21']\n",
        "\n",
        "train_df = train_df.sort_values(['id','cycle'])\n",
        "\n",
        "# read test data - specify the path for the test data\n",
        "test_df = pd.read_csv('/content/drive/MyDrive/PFC/FDIA-PdM/Datasets/Testing/PM_test.txt', sep=\" \", header=None)\n",
        "test_df.drop(test_df.columns[[26, 27]], axis=1, inplace=True)\n",
        "test_df.columns = ['id', 'cycle', 'setting1', 'setting2', 'setting3', 's1', 's2', 's3',\n",
        "                     's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14',\n",
        "                     's15', 's16', 's17', 's18', 's19', 's20', 's21']\n",
        "\n",
        "# read ground truth data - It contains the information of true remaining cycles for each engine in the testing data.\n",
        "truth_df = pd.read_csv('/content/drive/MyDrive/PFC/FDIA-PdM/Datasets/Testing/PM_truth.txt', sep=\" \", header=None)\n",
        "truth_df.drop(truth_df.columns[[1]], axis=1, inplace=True)\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pmNJTSMN6CN"
      },
      "source": [
        "# 4. Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M677GjcWOB-d"
      },
      "source": [
        "# 4.1 Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MKqEXAtN8we"
      },
      "source": [
        "# Data Labeling - generate column RUL(Remaining Usefull Life or Time to Failure)\n",
        "rul = pd.DataFrame(train_df.groupby('id')['cycle'].max()).reset_index()\n",
        "rul.columns = ['id', 'max']\n",
        "train_df = train_df.merge(rul, on=['id'], how='left')\n",
        "train_df['RUL'] = train_df['max'] - train_df['cycle']\n",
        "train_df.drop('max', axis=1, inplace=True)\n",
        "\n",
        "# generate label columns for training data\n",
        "# we will only make use of \"label1\" for binary classification, \n",
        "# while trying to answer the question: is a specific engine going to fail within w1 cycles?\n",
        "w1 = 30\n",
        "w0 = 15\n",
        "train_df['label1'] = np.where(train_df['RUL'] <= w1, 1, 0 )\n",
        "train_df['label2'] = train_df['label1']\n",
        "train_df.loc[train_df['RUL'] <= w0, 'label2'] = 2\n",
        "\n",
        "# MinMax normalization (from 0 to 1)\n",
        "train_df['cycle_norm'] = train_df['cycle']\n",
        "cols_normalize = train_df.columns.difference(['id','cycle','RUL','label1','label2'])\n",
        "min_max_scaler = preprocessing.MinMaxScaler()\n",
        "norm_train_df = pd.DataFrame(min_max_scaler.fit_transform(train_df[cols_normalize]), \n",
        "                             columns=cols_normalize, \n",
        "                             index=train_df.index)\n",
        "join_df = train_df[train_df.columns.difference(cols_normalize)].join(norm_train_df)\n",
        "train_df = join_df.reindex(columns = train_df.columns)\n",
        "\n",
        "#train_df.to_csv('../../Dataset/PredictiveTraining.csv', encoding='utf-8',index = None)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGpLljZ9ONsy"
      },
      "source": [
        "# 4.2 Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zg7ZMBUtOQ5Q",
        "outputId": "81acacc3-3a05-4c3c-b656-203ddef094a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# MinMax normalization (from 0 to 1)\n",
        "test_df['cycle_norm'] = test_df['cycle']\n",
        "norm_test_df = pd.DataFrame(min_max_scaler.transform(test_df[cols_normalize]), \n",
        "                            columns=cols_normalize, \n",
        "                            index=test_df.index)\n",
        "test_join_df = test_df[test_df.columns.difference(cols_normalize)].join(norm_test_df)\n",
        "test_df = test_join_df.reindex(columns = test_df.columns)\n",
        "test_df = test_df.reset_index(drop=True)\n",
        "print(test_df.head())\n",
        "\n",
        "# We use the ground truth dataset to generate labels for the test data.\n",
        "# generate column max for test data\n",
        "rul = pd.DataFrame(test_df.groupby('id')['cycle'].max()).reset_index()\n",
        "rul.columns = ['id', 'max']\n",
        "truth_df.columns = ['more']\n",
        "truth_df['id'] = truth_df.index + 1\n",
        "truth_df['max'] = rul['max'] + truth_df['more']\n",
        "truth_df.drop('more', axis=1, inplace=True)\n",
        "\n",
        "# generate RUL for test data\n",
        "test_df = test_df.merge(truth_df, on=['id'], how='left')\n",
        "test_df['RUL'] = test_df['max'] - test_df['cycle']\n",
        "test_df.drop('max', axis=1, inplace=True)\n",
        "\n",
        "# generate label columns w0 and w1 for test data\n",
        "test_df['label1'] = np.where(test_df['RUL'] <= w1, 1, 0 )\n",
        "test_df['label2'] = test_df['label1']\n",
        "test_df.loc[test_df['RUL'] <= w0, 'label2'] = 2\n",
        "\n",
        "#test_df.to_csv('../../Dataset/PredictiveManteinanceTest.csv', encoding='utf-8',index = None)\n",
        "\n",
        "# pick a large window size of 50 cycles\n",
        "sequence_length = 100\n",
        "\n",
        "# function to reshape features into (samples, time steps, features) \n",
        "def gen_sequence(id_df, seq_length, seq_cols):\n",
        "\n",
        "    data_matrix = id_df[seq_cols].values\n",
        "    num_elements = data_matrix.shape[0]\n",
        "\n",
        "    for start, stop in zip(range(0, num_elements-seq_length), range(seq_length, num_elements)):\n",
        "        yield data_matrix[start:stop, :]\n",
        "\n",
        "# pick the feature columns\n",
        "sensor_cols = ['s' + str(i) for i in range(1,22)]\n",
        "sequence_cols = ['setting1', 'setting2', 'setting3', 'cycle_norm']\n",
        "sequence_cols.extend(sensor_cols)\n",
        "\n",
        "# TODO for debug \n",
        "# val is a list of 192 - 50 = 142 bi-dimensional array (50 rows x 25 columns)\n",
        "val=list(gen_sequence(train_df[train_df['id']==1], sequence_length, sequence_cols))\n",
        "print(len(val))\n",
        "\n",
        "# generator for the sequences\n",
        "# transform each id of the train dataset in a sequence\n",
        "seq_gen = (list(gen_sequence(train_df[train_df['id']==id], sequence_length, sequence_cols)) \n",
        "           for id in train_df['id'].unique())\n",
        "\n",
        "# generate sequences and convert to numpy array\n",
        "seq_array = np.concatenate(list(seq_gen)).astype(np.float32)\n",
        "print(seq_array.shape)\n",
        "\n",
        "# function to generate labels\n",
        "def gen_labels(id_df, seq_length, label):\n",
        "\n",
        "    data_matrix = id_df[label].values\n",
        "    num_elements = data_matrix.shape[0]\n",
        "\n",
        "    return data_matrix[seq_length:num_elements, :]\n",
        "\n",
        "# generate labels\n",
        "label_gen = [gen_labels(train_df[train_df['id']==id], sequence_length, ['RUL']) \n",
        "             for id in train_df['id'].unique()]\n",
        "\n",
        "label_array = np.concatenate(label_gen).astype(np.float32)\n",
        "label_array.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   id  cycle  setting1  setting2  ...  s19       s20       s21  cycle_norm\n",
            "0   1      1  0.632184  0.750000  ...  0.0  0.558140  0.661834     0.00000\n",
            "1   1      2  0.344828  0.250000  ...  0.0  0.682171  0.686827     0.00277\n",
            "2   1      3  0.517241  0.583333  ...  0.0  0.728682  0.721348     0.00554\n",
            "3   1      4  0.741379  0.500000  ...  0.0  0.666667  0.662110     0.00831\n",
            "4   1      5  0.580460  0.500000  ...  0.0  0.658915  0.716377     0.01108\n",
            "\n",
            "[5 rows x 27 columns]\n",
            "92\n",
            "(10631, 100, 25)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10631, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqDMCsxYObqK"
      },
      "source": [
        "# 5. Modeling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOlvrHTOOeaI"
      },
      "source": [
        "def rmse(y_true, y_pred):\n",
        "    return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1))\n",
        "def r2_keras(y_true, y_pred):\n",
        "    \"\"\"Coefficient of Determination \n",
        "    \"\"\"\n",
        "    SS_res =  K.sum(K.square( y_true - y_pred ))\n",
        "    SS_tot = K.sum(K.square( y_true - K.mean(y_true) ) )\n",
        "    return ( 1 - SS_res/(SS_tot + K.epsilon()) )\n",
        "\n",
        "nb_features = seq_array.shape[2]\n",
        "nb_out = label_array.shape[1]\n",
        "\n",
        "cnn = Sequential()\n",
        "cnn.add(InputLayer(input_shape=(sequence_length, nb_features)))\n",
        "cnn.add(BatchNormalization(axis=-1))  #Scaling the data\n",
        "\n",
        "cnn.add(Conv1D(filters=64,\n",
        "               kernel_size=3,\n",
        "               padding=\"valid\",\n",
        "               activation=\"relu\",\n",
        "               kernel_regularizer='l2'\n",
        "               )\n",
        "       )\n",
        "# cnn.add(MaxPool1D(pool_size=2))\n",
        "# cnn.add(BatchNormalization(axis=-1))\n",
        "cnn.add(Conv1D(filters=64,\n",
        "               kernel_size=3,\n",
        "               padding=\"valid\",\n",
        "               activation=\"relu\",\n",
        "               kernel_regularizer='l2')\n",
        "       )\n",
        "cnn.add(Conv1D(filters=64,\n",
        "               kernel_size=3,\n",
        "               padding=\"valid\",\n",
        "               activation=\"relu\",\n",
        "               kernel_regularizer='l2')\n",
        "       )\n",
        "cnn.add(Conv1D(filters=64,\n",
        "               kernel_size=3,\n",
        "               padding=\"valid\",\n",
        "               activation=\"relu\",\n",
        "               kernel_regularizer='l2')\n",
        "       )\n",
        "# cnn.add(BatchNormalization(axis=-1))\n",
        "# cnn.add(MaxPool1D(pool_size=2))\n",
        "# cnn.add(BatchNormalization(axis=-1))\n",
        "\n",
        "cnn.add(Flatten())\n",
        "cnn.add(Dense(40))\n",
        "cnn.add(Activation('relu'))\n",
        "cnn.add(Dense(30))\n",
        "cnn.add(Activation('relu'))\n",
        "cnn.add(Dense(units=nb_out))\n",
        "cnn.add(Activation(\"relu\"))\n",
        "cnn.compile(loss='mean_squared_error', optimizer='rmsprop',metrics=[rmse,r2_keras])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGUO98MmOtzf"
      },
      "source": [
        "# 6. EVALUATE ON TEST DATA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lwu-ZSQIOwZB",
        "outputId": "c16cf9c4-5031-4cbb-c391-0d00d9902653",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# We pick the last sequence for each id in the test data\n",
        "seq_array_test_last = [test_df[test_df['id']==id][sequence_cols].values[-sequence_length:] \n",
        "                       for id in test_df['id'].unique() if len(test_df[test_df['id']==id]) >= sequence_length]\n",
        "\n",
        "seq_array_test_last = np.asarray(seq_array_test_last).astype(np.float32)\n",
        "print(\"seq_array_test_last\")\n",
        "#print(seq_array_test_last)\n",
        "print(seq_array_test_last.shape)\n",
        "\n",
        "# Similarly, we pick the labels\n",
        "#print(\"y_mask\")\n",
        "y_mask = [len(test_df[test_df['id']==id]) >= sequence_length for id in test_df['id'].unique()]\n",
        "label_array_test_last = test_df.groupby('id')['RUL'].nth(-1)[y_mask].values\n",
        "label_array_test_last = label_array_test_last.reshape(label_array_test_last.shape[0],1).astype(np.float32)\n",
        "print(label_array_test_last.shape)\n",
        "print(\"label_array_test_last\")\n",
        "print(label_array_test_last)\n",
        "\n",
        "# if best iteration's model was saved then load and use it\n",
        "if os.path.isfile(model_path):\n",
        "    cnn.load_weights(model_path)\n",
        "    #estimator = load_model(model_path,custom_objects={'r2_keras': r2_keras})\n",
        "\n",
        "    # test metrics\n",
        "    scores_test = cnn.evaluate(seq_array_test_last, label_array_test_last, verbose=2)\n",
        "    print('\\nRMSE: {}'.format(scores_test[1]))\n",
        "    print('\\nR^2: {}'.format(scores_test[2]))\n",
        "\n",
        "    y_pred_test = cnn.predict(seq_array_test_last)\n",
        "    y_true_test = label_array_test_last\n",
        "    print(\"Prediction\")\n",
        "    print(y_pred_test);\n",
        "    print(\"Truth\")\n",
        "    print(y_true_test);\n",
        "\n",
        "    df = pd.DataFrame(y_pred_test)\n",
        "\n",
        "    ## save to xlsx file\n",
        "\n",
        "    filepath1 = '../../Output/PdM_CNN_level.csv'\n",
        "\n",
        "    # Plot in blue color the predicted data and in green color the\n",
        "    # actual data to verify visually the accuracy of the model.\n",
        "    fig_verify = plt.figure(figsize=(100, 50))\n",
        "    plt.plot(y_pred_test, color=\"blue\")\n",
        "    plt.plot(y_true_test, color=\"green\")\n",
        "    plt.title('prediction')\n",
        "    plt.ylabel('RUL in cycles')\n",
        "    plt.xlabel('Engine ID')\n",
        "    plt.legend(['predicted', 'actual data'], loc='upper left')\n",
        "    plt.show()\n",
        "    fig_verify.savefig(\"../../Output/model_regression_verify.png\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "seq_array_test_last\n",
            "(70, 100, 25)\n",
            "(70, 1)\n",
            "label_array_test_last\n",
            "[[ 69.]\n",
            " [ 82.]\n",
            " [ 93.]\n",
            " [ 91.]\n",
            " [ 95.]\n",
            " [ 96.]\n",
            " [124.]\n",
            " [ 95.]\n",
            " [ 84.]\n",
            " [ 50.]\n",
            " [ 28.]\n",
            " [ 87.]\n",
            " [ 16.]\n",
            " [ 57.]\n",
            " [113.]\n",
            " [ 20.]\n",
            " [ 66.]\n",
            " [ 97.]\n",
            " [ 90.]\n",
            " [115.]\n",
            " [  8.]\n",
            " [ 48.]\n",
            " [  7.]\n",
            " [ 11.]\n",
            " [ 19.]\n",
            " [ 21.]\n",
            " [ 50.]\n",
            " [ 28.]\n",
            " [ 18.]\n",
            " [ 10.]\n",
            " [ 59.]\n",
            " [114.]\n",
            " [ 47.]\n",
            " [ 21.]\n",
            " [114.]\n",
            " [ 29.]\n",
            " [ 26.]\n",
            " [ 97.]\n",
            " [137.]\n",
            " [ 15.]\n",
            " [103.]\n",
            " [ 37.]\n",
            " [100.]\n",
            " [ 21.]\n",
            " [ 54.]\n",
            " [ 72.]\n",
            " [ 28.]\n",
            " [ 14.]\n",
            " [  8.]\n",
            " [ 94.]\n",
            " [ 50.]\n",
            " [131.]\n",
            " [126.]\n",
            " [ 10.]\n",
            " [ 34.]\n",
            " [ 63.]\n",
            " [ 90.]\n",
            " [  8.]\n",
            " [  9.]\n",
            " [ 58.]\n",
            " [ 89.]\n",
            " [136.]\n",
            " [ 28.]\n",
            " [ 38.]\n",
            " [ 20.]\n",
            " [ 85.]\n",
            " [ 55.]\n",
            " [ 82.]\n",
            " [ 59.]\n",
            " [ 20.]]\n"
          ]
        }
      ]
    }
  ]
}