{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "regression_SpeedingUpCNNs.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyM8x5XKrRume+mY6j2iu+aQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/masdesouza/FDIA-PdM/blob/master/notebooks/regression_SpeedingUpCNNs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5W-lCtST--u"
      },
      "source": [
        "#1. Import modules\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQP_7M4ghdY3"
      },
      "source": [
        "# Regression models: How many more cycles an in-service engine will last before it fails?\n",
        "import tensorflow.keras \n",
        "from tensorflow.keras.layers import Conv1D, BatchNormalization, Dropout, Dense, InputLayer, Flatten, MaxPool1D, Activation, GlobalAveragePooling1D, Conv2D,Conv3D,Lambda\n",
        "from tensorflow.keras.activations import relu, softmax\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.losses import binary_crossentropy\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.models import Sequential,load_model\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from sklearn import preprocessing"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYGmDLpGUqhv"
      },
      "source": [
        "#2. Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vI-RU6t2T-CY"
      },
      "source": [
        "# Setting seed for reproducibility\n",
        "np.random.seed(1234)  \n",
        "PYTHONHASHSEED = 0\n",
        "\n",
        "# define path to save model\n",
        "model_path = '/content/drive/MyDrive/PFC/FDIA-PdM/Trained models/CNN/regression_model_SpeedingUpCNNs.h5'\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raVt4XEMU-Rt"
      },
      "source": [
        "##2.1  Import dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMhX0JNMUvo3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b77ffee-64b2-46b1-cbf2-f7c2d1f132e2"
      },
      "source": [
        "import os\n",
        "import pickle\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "os.chdir(\"/content/drive/MyDrive\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioIgu1t1VMDa"
      },
      "source": [
        "#3. Data Ingestion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iIBaVJpU9Fd"
      },
      "source": [
        "# read training data - It is the aircraft engine run-to-failure data.\n",
        "train_df = pd.read_csv('/content/drive/MyDrive/PFC/FDIA-PdM/Datasets/CMAPSSData/train_FD002.txt', sep=\" \", header=None)\n",
        "train_df.drop(train_df.columns[[26, 27]], axis=1, inplace=True)\n",
        "train_df.columns = ['id', 'cycle', 'setting1', 'setting2', 'setting3', 's1', 's2', 's3',\n",
        "                     's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14',\n",
        "                     's15', 's16', 's17', 's18', 's19', 's20', 's21']\n",
        "\n",
        "train_df = train_df.sort_values(['id','cycle'])\n",
        "\n",
        "# read test data - It is the aircraft engine operating data without failure events recorded.\n",
        "test_df = pd.read_csv('//content/drive/MyDrive/PFC/FDIA-PdM/Datasets/CMAPSSData/test_FD002.txt', sep=\" \", header=None)\n",
        "test_df.drop(test_df.columns[[26, 27]], axis=1, inplace=True)\n",
        "test_df.columns = ['id', 'cycle', 'setting1', 'setting2', 'setting3', 's1', 's2', 's3',\n",
        "                     's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14',\n",
        "                     's15', 's16', 's17', 's18', 's19', 's20', 's21']\n",
        "\n",
        "# read ground truth data - It contains the information of true remaining cycles for each engine in the testing data.\n",
        "truth_df = pd.read_csv('/content/drive/MyDrive/PFC/FDIA-PdM/Datasets/CMAPSSData/RUL_FD002.txt', sep=\" \", header=None)\n",
        "truth_df.drop(truth_df.columns[[1]], axis=1, inplace=True)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Mb6fWj-byi5"
      },
      "source": [
        "#4. Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPL62MSxfxU0"
      },
      "source": [
        "## 4.1 Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59EtRS6rb2R_"
      },
      "source": [
        "# Data Labeling - generate column RUL(Remaining Usefull Life or Time to Failure)\n",
        "rul = pd.DataFrame(train_df.groupby('id')['cycle'].max()).reset_index()\n",
        "rul.columns = ['id', 'max']\n",
        "train_df = train_df.merge(rul, on=['id'], how='left')\n",
        "train_df['RUL'] = train_df['max'] - train_df['cycle']\n",
        "train_df.drop('max', axis=1, inplace=True)\n",
        "\n",
        "# generate label columns for training data\n",
        "# we will only make use of \"label1\" for binary classification, \n",
        "# while trying to answer the question: is a specific engine going to fail within w1 cycles?\n",
        "w1 = 30\n",
        "w0 = 15\n",
        "train_df['label1'] = np.where(train_df['RUL'] <= w1, 1, 0 )\n",
        "train_df['label2'] = train_df['label1']\n",
        "train_df.loc[train_df['RUL'] <= w0, 'label2'] = 2\n",
        "\n",
        "# MinMax normalization (from 0 to 1)\n",
        "train_df['cycle_norm'] = train_df['cycle']\n",
        "cols_normalize = train_df.columns.difference(['id','cycle','RUL','label1','label2'])\n",
        "min_max_scaler = preprocessing.MinMaxScaler()\n",
        "norm_train_df = pd.DataFrame(min_max_scaler.fit_transform(train_df[cols_normalize]), \n",
        "                             columns=cols_normalize, \n",
        "                             index=train_df.index)\n",
        "join_df = train_df[train_df.columns.difference(cols_normalize)].join(norm_train_df)\n",
        "train_df = join_df.reindex(columns = train_df.columns)\n",
        "\n",
        "#train_df.to_csv('../../Dataset/PredictiveTraining.csv', encoding='utf-8',index = None)\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2BELKRKgODN"
      },
      "source": [
        "## 4.2 Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZ_wwC_Uf4pT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18d2da93-3afc-4610-b96d-11564d69810a"
      },
      "source": [
        "# MinMax normalization (from 0 to 1)\n",
        "test_df['cycle_norm'] = test_df['cycle']\n",
        "norm_test_df = pd.DataFrame(min_max_scaler.transform(test_df[cols_normalize]), \n",
        "                            columns=cols_normalize, \n",
        "                            index=test_df.index)\n",
        "test_join_df = test_df[test_df.columns.difference(cols_normalize)].join(norm_test_df)\n",
        "test_df = test_join_df.reindex(columns = test_df.columns)\n",
        "test_df = test_df.reset_index(drop=True)\n",
        "print(test_df.head())\n",
        "\n",
        "# We use the ground truth dataset to generate labels for the test data.\n",
        "# generate column max for test data\n",
        "rul = pd.DataFrame(test_df.groupby('id')['cycle'].max()).reset_index()\n",
        "rul.columns = ['id', 'max']\n",
        "truth_df.columns = ['more']\n",
        "truth_df['id'] = truth_df.index + 1\n",
        "truth_df['max'] = rul['max'] + truth_df['more']\n",
        "truth_df.drop('more', axis=1, inplace=True)\n",
        "\n",
        "# generate RUL for test data\n",
        "test_df = test_df.merge(truth_df, on=['id'], how='left')\n",
        "test_df['RUL'] = test_df['max'] - test_df['cycle']\n",
        "test_df.drop('max', axis=1, inplace=True)\n",
        "\n",
        "# generate label columns w0 and w1 for test data\n",
        "test_df['label1'] = np.where(test_df['RUL'] <= w1, 1, 0 )\n",
        "test_df['label2'] = test_df['label1']\n",
        "test_df.loc[test_df['RUL'] <= w0, 'label2'] = 2\n",
        "\n",
        "#test_df.to_csv('../../Dataset/PredictiveManteinanceTest.csv', encoding='utf-8',index = None)\n",
        "\n",
        "# pick a large window size of 50 cycles\n",
        "sequence_length = 100\n",
        "\n",
        "# function to reshape features into (samples, time steps, features) \n",
        "def gen_sequence(id_df, seq_length, seq_cols):\n",
        "\n",
        "    data_matrix = id_df[seq_cols].values\n",
        "    num_elements = data_matrix.shape[0]\n",
        "\n",
        "    for start, stop in zip(range(0, num_elements-seq_length), range(seq_length, num_elements)):\n",
        "        yield data_matrix[start:stop, :]\n",
        "\n",
        "# pick the feature columns\n",
        "sensor_cols = ['s' + str(i) for i in range(1,22)]\n",
        "sequence_cols = ['setting1', 'setting2', 'setting3', 'cycle_norm']\n",
        "sequence_cols.extend(sensor_cols)\n",
        "\n",
        "# TODO for debug \n",
        "# val is a list of 192 - 50 = 142 bi-dimensional array (50 rows x 25 columns)\n",
        "val=list(gen_sequence(train_df[train_df['id']==1], sequence_length, sequence_cols))\n",
        "print(len(val))\n",
        "\n",
        "# generator for the sequences\n",
        "# transform each id of the train dataset in a sequence\n",
        "seq_gen = (list(gen_sequence(train_df[train_df['id']==id], sequence_length, sequence_cols)) \n",
        "           for id in train_df['id'].unique())\n",
        "\n",
        "# generate sequences and convert to numpy array\n",
        "seq_array = np.concatenate(list(seq_gen)).astype(np.float32)\n",
        "print(seq_array.shape)\n",
        "\n",
        "# function to generate labels\n",
        "def gen_labels(id_df, seq_length, label):\n",
        "\n",
        "    data_matrix = id_df[label].values\n",
        "    num_elements = data_matrix.shape[0]\n",
        "\n",
        "    return data_matrix[seq_length:num_elements, :]\n",
        "\n",
        "# generate labels\n",
        "label_gen = [gen_labels(train_df[train_df['id']==id], sequence_length, ['RUL']) \n",
        "             for id in train_df['id'].unique()]\n",
        "\n",
        "label_array = np.concatenate(label_gen).astype(np.float32)\n",
        "label_array.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   id  cycle  setting1  setting2  ...  s19       s20       s21  cycle_norm\n",
            "0   1      1  0.238019  0.297150  ...  1.0  0.625514  0.633951    0.000000\n",
            "1   1      2  0.476162  0.831354  ...  1.0  0.483882  0.500205    0.002653\n",
            "2   1      3  0.833282  0.997625  ...  1.0  0.164609  0.165078    0.005305\n",
            "3   1      4  0.999967  0.998812  ...  1.0  0.005830  0.023186    0.007958\n",
            "4   1      5  0.595089  0.737886  ...  0.0  0.141632  0.145822    0.010610\n",
            "\n",
            "[5 rows x 27 columns]\n",
            "49\n",
            "(27759, 100, 25)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(27759, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BICYNX1UgfvP"
      },
      "source": [
        "# 5. Modeling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGMkFQvHNhkB"
      },
      "source": [
        "ExpandDimension = lambda axis: Lambda(lambda x: K.expand_dims(x, axis))\n",
        "SqueezeDimension = lambda axis: Lambda(lambda x: K.squeeze(x, axis))\n",
        "\n",
        "# Layers\n",
        "def simple_factorized_conv(filters, kernel, *args, **kwargs):\n",
        "    kwargs[\"activation\"] = None\n",
        "    def __inner(inp):\n",
        "        cnn1 = Conv2D(filters, (kernel[0], 1), *args, **kwargs)(inp)\n",
        "        cnn2 = Conv2D(filters, (1, kernel[1]), *args, **kwargs)(cnn1)\n",
        "        \n",
        "        return cnn2\n",
        "\n",
        "    return __inner\n",
        "  \n",
        "def cp_decomposed_conv(filters, kernel, *args, **kwargs):\n",
        "    \"\"\"\n",
        "    Beware, it doesn't work! It's just an aproximate demostration of how\n",
        "    CP decomposition should be implemented. The issue is in the dimension matching.\n",
        "    If you found the solution, feel free to comment it, either in the blogpost or here.\n",
        "    You'll get full credit for your finding.\n",
        "    \"\"\"\n",
        "    kwargs[\"activation\"] = None\n",
        "    rank = filters // 2\n",
        "    d = kernel[0]\n",
        "    def __inner(inp):\n",
        "        first    = Conv2D(rank, kernel_size=(1, 1), **kwargs)(inp)\n",
        "        \n",
        "        expanded = ExpandDimension(axis=1)(first)\n",
        "        mid1     = Conv3D(rank, kernel_size=(d, 1, 1), **kwargs)(expanded)\n",
        "        mid2     = Conv3D(rank, kernel_size=(1, d, 1), **kwargs)(mid1)\n",
        "        squeezed = SqueezeDimension(axis=1)(mid2)\n",
        "        \n",
        "        last     = Conv2D(filters,  kernel_size=(1, 1), **kwargs)(squeezed)\n",
        "        \n",
        "        return last\n",
        "\n",
        "    return __inner\n",
        "\n",
        "def bn_relu(layer):\n",
        "    def __inner(*args, **kwargs):\n",
        "        l = layer(*args, **kwargs)\n",
        "        bn = BatchNormalization()(l)\n",
        "        act = Activation(\"relu\")(bn)\n",
        "        \n",
        "        return act # courtesy to Jiun-Kuei Jung\n",
        "\n",
        "    return __inner"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLfYRA3-NpRE"
      },
      "source": [
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import (\n",
        "    Dense, Dropout, Conv2D, SeparableConv2D,\n",
        "    MaxPool2D, Flatten, GlobalAveragePooling2D\n",
        ")\n",
        "\n",
        "class BaseExampleModel:\n",
        "    def __init__(self, conv_type=\"std\"):\n",
        "        \"\"\"\n",
        "        :convtype: used as a flag to easily switch the Convolution layer implementation\n",
        "        \"\"\"\n",
        "        self.conv_layer = lambda *args, **kwargs: {\n",
        "                              \"std\": bn_relu(Conv2D(*args, **kwargs)),\n",
        "                              \"sep\": bn_relu(SeparableConv2D(*args, **kwargs)),\n",
        "                              \"cp\" : bn_relu(cp_decomposed_conv(*args, **kwargs)),\n",
        "                              \"fac\": bn_relu(simple_factorized_conv(*args, **kwargs))\n",
        "                          }[conv_type]\n",
        "        \n",
        "    def make(self, inp, nb_classes):\n",
        "        \"\"\"\n",
        "        :inp: a reference to the Keras Input layer, to easily switch the input size\n",
        "              and even the way data is fed into network\n",
        "              (via standard Keras methods or via TF Dataset API)\n",
        "        :nb_classes: for the final Dense layer\n",
        "        \"\"\"\n",
        "        NotImplemented\n",
        "\n",
        "class AllCNNLike(BaseExampleModel):\n",
        "    \"\"\"\n",
        "    isnpired from https://arxiv.org/abs/1412.6806\n",
        "    namely All-CNN-C\n",
        "    \"\"\"\n",
        "    def __init__(self, conv_type=\"std\"):\n",
        "        super().__init__(conv_type=conv_type)\n",
        "        \n",
        "    def make(self, inp, nb_classes):\n",
        "        conv1 = self.conv_layer(96, (3, 3), padding=\"same\")(inp)\n",
        "        conv2 = self.conv_layer(96, (3, 3), padding=\"same\")(conv1)\n",
        "        \n",
        "        conv3 = bn_relu(Conv2D(96, (3, 3), padding=\"same\", strides=(2, 2)))(conv2)\n",
        "        \n",
        "        conv4 = self.conv_layer(192, (3, 3), padding=\"same\")(conv3)\n",
        "        conv5 = self.conv_layer(192, (3, 3), padding=\"same\")(conv4)\n",
        "        \n",
        "        conv6 = bn_relu(Conv2D(192, (3, 3), padding=\"same\", strides=(2, 2)))(conv5)\n",
        "        \n",
        "        conv7 = bn_relu(Conv2D(192, (3, 3), padding=\"same\"))(conv6)\n",
        "        conv8 = bn_relu(Conv2D(192, (1, 1), padding=\"same\"))(conv7)\n",
        "        conv9 = bn_relu(Conv2D(nb_classes, (1, 1), padding=\"same\"))(conv8)\n",
        "        \n",
        "        gap = GlobalAveragePooling2D()(conv9)\n",
        "        final = Activation(\"softmax\")(gap)\n",
        "        \n",
        "        return Model(inputs=inp, outputs=final)\n",
        "      \n",
        "class WiderAllCNNLike(BaseExampleModel):\n",
        "    \"\"\"\n",
        "    isnpired from https://arxiv.org/abs/1412.6806\n",
        "    namely All-CNN-C\n",
        "    \"\"\"\n",
        "    def __init__(self, conv_type=\"std\"):\n",
        "        super().__init__(conv_type=conv_type)\n",
        "        \n",
        "    def make(self, inp, nb_classes):\n",
        "        conv1 = self.conv_layer(192, (3, 3), padding=\"same\")(inp)\n",
        "        \n",
        "        conv2 = bn_relu(Conv2D(96, (3, 3), padding=\"same\", strides=(2, 2)))(conv1)\n",
        "        \n",
        "        conv3 = self.conv_layer(384, (3, 3), padding=\"same\")(conv2)\n",
        "        \n",
        "        conv4 = bn_relu(Conv2D(128, (3, 3), padding=\"same\", strides=(2, 2)))(conv3)\n",
        "        \n",
        "        conv5 = bn_relu(Conv2D(256, (3, 3), padding=\"same\"))(conv4)\n",
        "        conv6 = bn_relu(Conv2D(nb_classes, (1, 1), padding=\"same\"))(conv5)\n",
        "        \n",
        "        gap = GlobalAveragePooling2D()(conv6)\n",
        "        final = Activation(\"softmax\")(gap)\n",
        "        \n",
        "        return Model(inputs=inp, outputs=final)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wHAMJ1-NyyF"
      },
      "source": [
        "from tensorflow.keras.metrics import categorical_accuracy, top_k_categorical_accuracy\n",
        "from tensorflow.keras.optimizers import  SGD\n",
        "from tensorflow.keras.layers import Input\n",
        "\n",
        "def main(conv_type=\"std\", verbose=False, net_type=\"simple\"):\n",
        "    inp = Input(shape=(32, 32, 3))\n",
        "    net = {\n",
        "        \"simple\": AllCNNLike(conv_type=conv_type).make(inp, nb_classes=100),\n",
        "        \"wide\": WiderAllCNNLike(conv_type=conv_type).make(inp, nb_classes=100)\n",
        "    }[net_type]\n",
        "\n",
        "    net.compile(SGD(lr=0.01, decay=0.001, momentum=0.8), \"categorical_crossentropy\",\n",
        "                 metrics=[categorical_accuracy, top_k_categorical_accuracy])\n",
        "\n",
        "    if verbose:\n",
        "        print(net.summary())\n",
        "\n",
        "    net.fit_generator(train_df, steps_per_epoch=50000 // BATCH_SIZE, epochs=5, verbose=verbose)\n",
        "    loss, acc, topk_acc = net.evaluate_generator(test_df, steps=10000 // BATCH_SIZE)\n",
        "\n",
        "    print(f\"Accuracy: {acc}, Top5 Accuracy {topk_acc} and Loss {loss}.\")"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3FLziWkgNh5"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def r2_keras(y_true, y_pred):\n",
        "    \"\"\"Coefficient of Determination \n",
        "    \"\"\"\n",
        "    SS_res =  K.sum(K.square( y_true - y_pred ))\n",
        "    SS_tot = K.sum(K.square( y_true - K.mean(y_true) ) )\n",
        "    return ( 1 - SS_res/(SS_tot + K.epsilon()) )\n",
        "\n",
        "def rmse(y_true, y_pred):\n",
        "    return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1))\n",
        "# Next, we build a deep network. \n",
        "# The first layer is an LSTM layer with 100 units followed by another LSTM layer with 50 units. \n",
        "\n",
        "nb_features = seq_array.shape[2]\n",
        "nb_out = label_array.shape[1]\n",
        "\n",
        "cnn = Sequential()\n",
        "cnn.add(InputLayer(input_shape=(sequence_length, nb_features)))\n",
        "cnn.add(BatchNormalization(axis=-1))  #Scaling the data\n",
        "\n",
        "cnn.add(Conv1D(filters=64,\n",
        "               kernel_size=3,\n",
        "               padding=\"valid\",\n",
        "               activation=\"relu\",\n",
        "               kernel_regularizer='l2'\n",
        "               )\n",
        "       )\n",
        "# cnn.add(MaxPool1D(pool_size=2))\n",
        "# cnn.add(BatchNormalization(axis=-1))\n",
        "cnn.add(Conv1D(filters=64,\n",
        "               kernel_size=3,\n",
        "               padding=\"valid\",\n",
        "               activation=\"relu\",\n",
        "               kernel_regularizer='l2')\n",
        "       )\n",
        "\n",
        "cnn.add(Conv1D(filters=64,\n",
        "               kernel_size=3,\n",
        "               padding=\"valid\",\n",
        "               activation=\"relu\",\n",
        "               kernel_regularizer='l2')\n",
        "       )\n",
        "# cnn.add(BatchNormalization(axis=-1))\n",
        "# cnn.add(MaxPool1D(pool_size=2))\n",
        "# cnn.add(BatchNormalization(axis=-1))\n",
        "\n",
        "cnn.add(Conv1D(filters=64,\n",
        "               kernel_size=3,\n",
        "               padding=\"valid\",\n",
        "               activation=\"relu\",\n",
        "               kernel_regularizer='l2')\n",
        "       )\n",
        "\n",
        "cnn.add(Flatten())\n",
        "cnn.add(Dense(40))\n",
        "cnn.add(Activation('relu'))\n",
        "cnn.add(Dense(30))\n",
        "cnn.add(Activation('relu'))\n",
        "cnn.add(Dense(units=nb_out))\n",
        "cnn.add(Activation(\"relu\"))\n",
        "# cnn.add(Dense(30))\n",
        "# cnn.add(Activation('relu'))\n",
        "# cnn.add(Dense(20))\n",
        "# cnn.add(Activation('relu'))\n",
        "# cnn.add(Dense(10))\n",
        "# cnn.add(Activation('relu'))\n",
        "# cnn.add(Dense(units=nb_out))\n",
        "# cnn.add(Activation('relu'))\n",
        "cnn.compile(loss='mean_squared_error', optimizer='rmsprop',metrics=[rmse,r2_keras])\n",
        "\n",
        "print(cnn.summary())\n",
        "\n",
        "history = cnn.fit(seq_array, label_array, epochs=100, batch_size=512, validation_split=0.05, verbose=2,\n",
        "          callbacks = [tensorflow.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=0, mode='min'),\n",
        "                       tensorflow.keras.callbacks.ModelCheckpoint(model_path,monitor='val_loss', save_best_only=True, mode='min', verbose=0)]\n",
        "    )\n",
        "\n",
        "# model = Sequential()\n",
        "# model.add(LSTM(\n",
        "#          input_shape=(sequence_length, nb_features),\n",
        "#          units=100,\n",
        "#          return_sequences=True))\n",
        "# model.add(Dropout(0.2))\n",
        "# model.add(LSTM(\n",
        "#           units=100,\n",
        "#           return_sequences=False))\n",
        "# model.add(Dropout(0.2))\n",
        "# model.add(Dense(units=nb_out))\n",
        "# model.add(Activation(\"linear\"))\n",
        "# model.compile(loss='mean_squared_error', optimizer='rmsprop',metrics=['mae',r2_keras])\n",
        "#\n",
        "# print(model.summary())\n",
        "#\n",
        "#\n",
        "#\n",
        "# # fit the network\n",
        "# history = model.fit(seq_array, label_array, epochs=100, batch_size=200, validation_split=0.05, verbose=2,\n",
        "#           callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=0, mode='min'),\n",
        "#                        keras.callbacks.ModelCheckpoint(model_path,monitor='val_loss', save_best_only=True, mode='min', verbose=0)]\n",
        "#           )\n",
        "\n",
        "# list all data in history\n",
        "print(history.history.keys())\n",
        "\n",
        "\n",
        "# training metrics\n",
        "scores = cnn.evaluate(seq_array, label_array, verbose=1, batch_size=200)\n",
        "print('\\nMAE: {}'.format(scores[1]))\n",
        "print('\\nR^2: {}'.format(scores[2]))\n",
        "\n",
        "y_pred = cnn.predict(seq_array,verbose=1, batch_size=200)\n",
        "y_true = label_array\n",
        "\n",
        "test_set = pd.DataFrame(y_pred)\n",
        "test_set.to_csv('/content/drive/MyDrive/PFC/FDIA-PdM/Output/submit_train_SpeedingUpCNNs.csv', index = None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-AupfE_hg4J"
      },
      "source": [
        "#6. EVALUATE ON TEST DATA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKlGVtWBhgNx",
        "outputId": "571a05e4-c7ec-49d8-f305-96e509f9ad0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# We pick the last sequence for each id in the test data\n",
        "seq_array_test_last = [test_df[test_df['id']==id][sequence_cols].values[-sequence_length:] \n",
        "                       for id in test_df['id'].unique() if len(test_df[test_df['id']==id]) >= sequence_length]\n",
        "\n",
        "seq_array_test_last = np.asarray(seq_array_test_last).astype(np.float32)\n",
        "print(\"seq_array_test_last\")\n",
        "#print(seq_array_test_last)\n",
        "print(seq_array_test_last.shape)\n",
        "\n",
        "# Similarly, we pick the labels\n",
        "#print(\"y_mask\")\n",
        "y_mask = [len(test_df[test_df['id']==id]) >= sequence_length for id in test_df['id'].unique()]\n",
        "label_array_test_last = test_df.groupby('id')['RUL'].nth(-1)[y_mask].values\n",
        "label_array_test_last = label_array_test_last.reshape(label_array_test_last.shape[0],1).astype(np.float32)\n",
        "print(label_array_test_last.shape)\n",
        "print(\"label_array_test_last\")\n",
        "print(label_array_test_last)\n",
        "\n",
        "# if best iteration's model was saved then load and use it\n",
        "if os.path.isfile(model_path):\n",
        "    cnn.load_weights(model_path)\n",
        "    #estimator = load_model(model_path,custom_objects={'r2_keras': r2_keras})\n",
        "\n",
        "    # test metrics\n",
        "    scores_test = cnn.evaluate(seq_array_test_last, label_array_test_last, verbose=2)\n",
        "    print('\\nMAE: {}'.format(scores_test[1]))\n",
        "    print('\\nR^2: {}'.format(scores_test[2]))\n",
        "\n",
        "    y_pred_test = cnn.predict(seq_array_test_last)\n",
        "    y_true_test = label_array_test_last\n",
        "    print(\"Prediction\")\n",
        "    print(y_pred_test);\n",
        "    print(\"Truth\")\n",
        "    print(y_true_test);\n",
        "\n",
        "    # Plot in blue color the predicted data and in green color the\n",
        "    # actual data to verify visually the accuracy of the model.\n",
        "    fig_verify = plt.figure(figsize=(100, 50))\n",
        "    plt.plot(y_pred_test, color=\"blue\")\n",
        "    plt.plot(y_true_test, color=\"green\")\n",
        "    plt.title('prediction')\n",
        "    plt.ylabel('RUL in cycles')\n",
        "    plt.xlabel('Engine ID')\n",
        "    plt.legend(['predicted', 'actual data'], loc='upper left')\n",
        "    plt.show()\n",
        "    fig_verify.savefig(\"/content/drive/MyDrive/PFC/FDIA-PdM/Output/model_regression_verify_CNN.png\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "seq_array_test_last\n",
            "(168, 100, 25)\n",
            "(168, 1)\n",
            "label_array_test_last\n",
            "[[ 18.]\n",
            " [106.]\n",
            " [ 15.]\n",
            " [  6.]\n",
            " [ 90.]\n",
            " [ 11.]\n",
            " [  6.]\n",
            " [ 30.]\n",
            " [ 11.]\n",
            " [ 37.]\n",
            " [ 68.]\n",
            " [ 22.]\n",
            " [ 54.]\n",
            " [ 97.]\n",
            " [ 10.]\n",
            " [ 77.]\n",
            " [ 88.]\n",
            " [ 83.]\n",
            " [ 78.]\n",
            " [ 75.]\n",
            " [ 11.]\n",
            " [ 53.]\n",
            " [100.]\n",
            " [ 55.]\n",
            " [ 48.]\n",
            " [ 37.]\n",
            " [ 44.]\n",
            " [ 27.]\n",
            " [ 18.]\n",
            " [  6.]\n",
            " [ 15.]\n",
            " [112.]\n",
            " [ 13.]\n",
            " [ 13.]\n",
            " [ 53.]\n",
            " [ 52.]\n",
            " [103.]\n",
            " [ 26.]\n",
            " [ 73.]\n",
            " [ 39.]\n",
            " [ 39.]\n",
            " [ 14.]\n",
            " [ 11.]\n",
            " [ 86.]\n",
            " [ 56.]\n",
            " [115.]\n",
            " [ 17.]\n",
            " [104.]\n",
            " [ 78.]\n",
            " [ 86.]\n",
            " [ 98.]\n",
            " [ 36.]\n",
            " [ 52.]\n",
            " [ 91.]\n",
            " [ 15.]\n",
            " [ 17.]\n",
            " [ 47.]\n",
            " [ 21.]\n",
            " [ 79.]\n",
            " [  8.]\n",
            " [  9.]\n",
            " [ 73.]\n",
            " [ 97.]\n",
            " [ 49.]\n",
            " [ 31.]\n",
            " [ 97.]\n",
            " [  9.]\n",
            " [ 14.]\n",
            " [  8.]\n",
            " [  8.]\n",
            " [106.]\n",
            " [ 61.]\n",
            " [168.]\n",
            " [ 35.]\n",
            " [ 80.]\n",
            " [  9.]\n",
            " [ 50.]\n",
            " [ 78.]\n",
            " [ 91.]\n",
            " [  7.]\n",
            " [106.]\n",
            " [ 15.]\n",
            " [  7.]\n",
            " [ 82.]\n",
            " [108.]\n",
            " [ 79.]\n",
            " [121.]\n",
            " [ 39.]\n",
            " [ 38.]\n",
            " [  9.]\n",
            " [167.]\n",
            " [ 88.]\n",
            " [  7.]\n",
            " [ 51.]\n",
            " [ 55.]\n",
            " [ 47.]\n",
            " [ 81.]\n",
            " [ 43.]\n",
            " [ 98.]\n",
            " [ 10.]\n",
            " [ 11.]\n",
            " [ 34.]\n",
            " [115.]\n",
            " [ 99.]\n",
            " [171.]\n",
            " [ 15.]\n",
            " [  9.]\n",
            " [ 42.]\n",
            " [ 13.]\n",
            " [ 41.]\n",
            " [ 88.]\n",
            " [ 14.]\n",
            " [ 82.]\n",
            " [ 36.]\n",
            " [107.]\n",
            " [ 14.]\n",
            " [ 23.]\n",
            " [  6.]\n",
            " [ 35.]\n",
            " [ 97.]\n",
            " [ 68.]\n",
            " [ 14.]\n",
            " [ 67.]\n",
            " [ 19.]\n",
            " [ 10.]\n",
            " [ 43.]\n",
            " [ 12.]\n",
            " [ 13.]\n",
            " [ 37.]\n",
            " [ 80.]\n",
            " [ 93.]\n",
            " [ 32.]\n",
            " [103.]\n",
            " [ 68.]\n",
            " [192.]\n",
            " [ 48.]\n",
            " [ 85.]\n",
            " [  8.]\n",
            " [ 23.]\n",
            " [  8.]\n",
            " [  6.]\n",
            " [ 57.]\n",
            " [ 83.]\n",
            " [ 81.]\n",
            " [ 73.]\n",
            " [ 75.]\n",
            " [ 11.]\n",
            " [108.]\n",
            " [ 14.]\n",
            " [ 61.]\n",
            " [ 85.]\n",
            " [  8.]\n",
            " [101.]\n",
            " [ 89.]\n",
            " [190.]\n",
            " [ 12.]\n",
            " [ 62.]\n",
            " [101.]\n",
            " [ 17.]\n",
            " [ 16.]\n",
            " [ 56.]\n",
            " [ 23.]\n",
            " [ 12.]\n",
            " [ 43.]\n",
            " [ 48.]\n",
            " [122.]\n",
            " [ 56.]\n",
            " [ 51.]]\n",
            "6/6 - 0s - loss: 438.0678 - rmse: 15.0665 - r2_keras: 0.6213 - 37ms/epoch - 6ms/step\n",
            "\n",
            "MAE: 15.066495895385742\n",
            "\n",
            "R^2: 0.6212811470031738\n",
            "Prediction\n",
            "[[ 12.010989 ]\n",
            " [ 87.54855  ]\n",
            " [ 14.630079 ]\n",
            " [ 14.468227 ]\n",
            " [ 67.60443  ]\n",
            " [ 12.755656 ]\n",
            " [ 10.820574 ]\n",
            " [ 37.31399  ]\n",
            " [ 11.78145  ]\n",
            " [ 35.43728  ]\n",
            " [ 84.58148  ]\n",
            " [ 16.47624  ]\n",
            " [103.46584  ]\n",
            " [ 90.51696  ]\n",
            " [ 13.350316 ]\n",
            " [106.67843  ]\n",
            " [ 96.633606 ]\n",
            " [ 99.80424  ]\n",
            " [ 83.86205  ]\n",
            " [115.105446 ]\n",
            " [ 12.008129 ]\n",
            " [ 65.38258  ]\n",
            " [ 88.32525  ]\n",
            " [ 57.199825 ]\n",
            " [ 54.771893 ]\n",
            " [ 32.93979  ]\n",
            " [ 28.740969 ]\n",
            " [ 36.2378   ]\n",
            " [ 20.354265 ]\n",
            " [  8.246755 ]\n",
            " [ 11.453413 ]\n",
            " [100.38666  ]\n",
            " [ 31.35122  ]\n",
            " [ 13.75535  ]\n",
            " [ 46.096638 ]\n",
            " [ 47.248802 ]\n",
            " [ 96.68443  ]\n",
            " [ 29.512184 ]\n",
            " [ 67.91323  ]\n",
            " [ 19.826593 ]\n",
            " [ 60.14776  ]\n",
            " [ 18.31386  ]\n",
            " [ 38.7416   ]\n",
            " [122.12668  ]\n",
            " [ 77.17488  ]\n",
            " [128.88055  ]\n",
            " [ 15.332598 ]\n",
            " [ 92.718414 ]\n",
            " [ 83.68695  ]\n",
            " [137.23932  ]\n",
            " [126.97868  ]\n",
            " [ 29.464314 ]\n",
            " [ 72.06813  ]\n",
            " [127.35895  ]\n",
            " [ 11.196863 ]\n",
            " [ 25.33196  ]\n",
            " [ 33.72373  ]\n",
            " [ 35.880596 ]\n",
            " [ 81.09033  ]\n",
            " [  8.988203 ]\n",
            " [ 31.897276 ]\n",
            " [ 86.960945 ]\n",
            " [ 85.132355 ]\n",
            " [ 62.577896 ]\n",
            " [ 20.775505 ]\n",
            " [116.69282  ]\n",
            " [  4.401082 ]\n",
            " [ 16.131971 ]\n",
            " [ 35.899418 ]\n",
            " [ 13.825726 ]\n",
            " [114.78867  ]\n",
            " [ 63.09888  ]\n",
            " [148.22838  ]\n",
            " [ 32.80715  ]\n",
            " [109.416664 ]\n",
            " [ 20.267963 ]\n",
            " [ 63.612442 ]\n",
            " [101.320366 ]\n",
            " [ 86.5379   ]\n",
            " [ 13.82206  ]\n",
            " [114.01253  ]\n",
            " [ 27.529507 ]\n",
            " [ 16.49986  ]\n",
            " [ 60.32443  ]\n",
            " [ 97.00798  ]\n",
            " [102.24898  ]\n",
            " [113.432236 ]\n",
            " [ 46.824627 ]\n",
            " [ 66.29544  ]\n",
            " [ 12.873355 ]\n",
            " [147.11745  ]\n",
            " [ 99.02261  ]\n",
            " [ 10.72774  ]\n",
            " [ 90.50586  ]\n",
            " [ 52.762016 ]\n",
            " [ 63.06857  ]\n",
            " [ 85.97974  ]\n",
            " [ 61.894382 ]\n",
            " [115.723915 ]\n",
            " [ 15.605653 ]\n",
            " [ 11.094538 ]\n",
            " [ 37.972416 ]\n",
            " [ 77.482605 ]\n",
            " [119.02862  ]\n",
            " [160.89676  ]\n",
            " [ 19.15117  ]\n",
            " [ 36.093666 ]\n",
            " [ 58.06586  ]\n",
            " [ 11.644388 ]\n",
            " [ 43.63485  ]\n",
            " [ 87.84413  ]\n",
            " [ 12.822468 ]\n",
            " [107.87496  ]\n",
            " [ 41.59413  ]\n",
            " [ 87.59699  ]\n",
            " [ 20.1375   ]\n",
            " [ 35.3154   ]\n",
            " [  9.778013 ]\n",
            " [ 65.356094 ]\n",
            " [ 86.92861  ]\n",
            " [ 56.65595  ]\n",
            " [ 32.161068 ]\n",
            " [ 91.15155  ]\n",
            " [ 30.27597  ]\n",
            " [ 10.3695965]\n",
            " [ 74.55173  ]\n",
            " [ 28.281576 ]\n",
            " [ 12.665669 ]\n",
            " [ 21.522514 ]\n",
            " [ 79.424385 ]\n",
            " [122.55971  ]\n",
            " [ 47.078957 ]\n",
            " [171.33966  ]\n",
            " [120.39253  ]\n",
            " [158.54535  ]\n",
            " [ 53.38482  ]\n",
            " [ 65.58321  ]\n",
            " [  5.0250998]\n",
            " [ 42.168606 ]\n",
            " [ 16.387203 ]\n",
            " [ 22.892529 ]\n",
            " [ 75.05947  ]\n",
            " [ 55.243736 ]\n",
            " [109.58959  ]\n",
            " [103.983795 ]\n",
            " [ 57.833813 ]\n",
            " [ 22.921518 ]\n",
            " [118.67654  ]\n",
            " [ 12.239734 ]\n",
            " [100.77298  ]\n",
            " [153.20483  ]\n",
            " [  5.860498 ]\n",
            " [ 83.31751  ]\n",
            " [125.4924   ]\n",
            " [105.50683  ]\n",
            " [ 18.229052 ]\n",
            " [ 45.903316 ]\n",
            " [116.76534  ]\n",
            " [ 20.233368 ]\n",
            " [ 13.889951 ]\n",
            " [ 78.03191  ]\n",
            " [ 45.802025 ]\n",
            " [ 14.173051 ]\n",
            " [ 59.10895  ]\n",
            " [ 54.582714 ]\n",
            " [154.0599   ]\n",
            " [ 78.19681  ]\n",
            " [121.85509  ]]\n",
            "Truth\n",
            "[[ 18.]\n",
            " [106.]\n",
            " [ 15.]\n",
            " [  6.]\n",
            " [ 90.]\n",
            " [ 11.]\n",
            " [  6.]\n",
            " [ 30.]\n",
            " [ 11.]\n",
            " [ 37.]\n",
            " [ 68.]\n",
            " [ 22.]\n",
            " [ 54.]\n",
            " [ 97.]\n",
            " [ 10.]\n",
            " [ 77.]\n",
            " [ 88.]\n",
            " [ 83.]\n",
            " [ 78.]\n",
            " [ 75.]\n",
            " [ 11.]\n",
            " [ 53.]\n",
            " [100.]\n",
            " [ 55.]\n",
            " [ 48.]\n",
            " [ 37.]\n",
            " [ 44.]\n",
            " [ 27.]\n",
            " [ 18.]\n",
            " [  6.]\n",
            " [ 15.]\n",
            " [112.]\n",
            " [ 13.]\n",
            " [ 13.]\n",
            " [ 53.]\n",
            " [ 52.]\n",
            " [103.]\n",
            " [ 26.]\n",
            " [ 73.]\n",
            " [ 39.]\n",
            " [ 39.]\n",
            " [ 14.]\n",
            " [ 11.]\n",
            " [ 86.]\n",
            " [ 56.]\n",
            " [115.]\n",
            " [ 17.]\n",
            " [104.]\n",
            " [ 78.]\n",
            " [ 86.]\n",
            " [ 98.]\n",
            " [ 36.]\n",
            " [ 52.]\n",
            " [ 91.]\n",
            " [ 15.]\n",
            " [ 17.]\n",
            " [ 47.]\n",
            " [ 21.]\n",
            " [ 79.]\n",
            " [  8.]\n",
            " [  9.]\n",
            " [ 73.]\n",
            " [ 97.]\n",
            " [ 49.]\n",
            " [ 31.]\n",
            " [ 97.]\n",
            " [  9.]\n",
            " [ 14.]\n",
            " [  8.]\n",
            " [  8.]\n",
            " [106.]\n",
            " [ 61.]\n",
            " [168.]\n",
            " [ 35.]\n",
            " [ 80.]\n",
            " [  9.]\n",
            " [ 50.]\n",
            " [ 78.]\n",
            " [ 91.]\n",
            " [  7.]\n",
            " [106.]\n",
            " [ 15.]\n",
            " [  7.]\n",
            " [ 82.]\n",
            " [108.]\n",
            " [ 79.]\n",
            " [121.]\n",
            " [ 39.]\n",
            " [ 38.]\n",
            " [  9.]\n",
            " [167.]\n",
            " [ 88.]\n",
            " [  7.]\n",
            " [ 51.]\n",
            " [ 55.]\n",
            " [ 47.]\n",
            " [ 81.]\n",
            " [ 43.]\n",
            " [ 98.]\n",
            " [ 10.]\n",
            " [ 11.]\n",
            " [ 34.]\n",
            " [115.]\n",
            " [ 99.]\n",
            " [171.]\n",
            " [ 15.]\n",
            " [  9.]\n",
            " [ 42.]\n",
            " [ 13.]\n",
            " [ 41.]\n",
            " [ 88.]\n",
            " [ 14.]\n",
            " [ 82.]\n",
            " [ 36.]\n",
            " [107.]\n",
            " [ 14.]\n",
            " [ 23.]\n",
            " [  6.]\n",
            " [ 35.]\n",
            " [ 97.]\n",
            " [ 68.]\n",
            " [ 14.]\n",
            " [ 67.]\n",
            " [ 19.]\n",
            " [ 10.]\n",
            " [ 43.]\n",
            " [ 12.]\n",
            " [ 13.]\n",
            " [ 37.]\n",
            " [ 80.]\n",
            " [ 93.]\n",
            " [ 32.]\n",
            " [103.]\n",
            " [ 68.]\n",
            " [192.]\n",
            " [ 48.]\n",
            " [ 85.]\n",
            " [  8.]\n",
            " [ 23.]\n",
            " [  8.]\n",
            " [  6.]\n",
            " [ 57.]\n",
            " [ 83.]\n",
            " [ 81.]\n",
            " [ 73.]\n",
            " [ 75.]\n",
            " [ 11.]\n",
            " [108.]\n",
            " [ 14.]\n",
            " [ 61.]\n",
            " [ 85.]\n",
            " [  8.]\n",
            " [101.]\n",
            " [ 89.]\n",
            " [190.]\n",
            " [ 12.]\n",
            " [ 62.]\n",
            " [101.]\n",
            " [ 17.]\n",
            " [ 16.]\n",
            " [ 56.]\n",
            " [ 23.]\n",
            " [ 12.]\n",
            " [ 43.]\n",
            " [ 48.]\n",
            " [122.]\n",
            " [ 56.]\n",
            " [ 51.]]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-9b7e44276734>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Engine ID'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'predicted'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'actual data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'upper left'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m     \u001b[0mfig_verify\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/PFC/FDIA-PdM/Output/model_regression_verify_CNN.png\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    270\u001b[0m     \"\"\"\n\u001b[1;32m    271\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0m_show\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_show\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/pylab/backend_inline.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(close, block)\u001b[0m\n\u001b[1;32m     37\u001b[0m             display(\n\u001b[1;32m     38\u001b[0m                 \u001b[0mfigure_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                 \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_fetch_figure_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigure_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m             )\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36mdisplay\u001b[0;34m(*objs, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m                 \u001b[0;31m# kwarg-specified metadata gets precedence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m                 \u001b[0m_merge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmd_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m             \u001b[0mpublish_display_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mformat_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmd_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdisplay_id\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDisplayHandle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisplay_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36mpublish_display_data\u001b[0;34m(data, metadata, source, **kwargs)\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m     )\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/zmqshell.py\u001b[0m in \u001b[0;36mpublish\u001b[0;34m(self, data, metadata, source, transient, update)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         self.session.send(\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpub_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mident\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopic\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m         )\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, stream, msg_or_type, content, parent, ident, buffers, track, header, metadata)\u001b[0m\n\u001b[1;32m    735\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madapt_version\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madapt_version\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 737\u001b[0;31m         \u001b[0mto_send\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mident\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    738\u001b[0m         \u001b[0mto_send\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m         \u001b[0mlongest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mto_send\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mserialize\u001b[0;34m(self, msg, ident)\u001b[0m\n\u001b[1;32m    623\u001b[0m             \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m             \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m             \u001b[0;31m# content is already packed, as in a relayed message\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;31m# disallow nan, because it's not actually valid JSON\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m json_packer = lambda obj: jsonapi.dumps(obj, default=date_default,\n\u001b[0;32m--> 103\u001b[0;31m     \u001b[0mensure_ascii\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_nan\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m )\n\u001b[1;32m    105\u001b[0m \u001b[0mjson_unpacker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mjsonapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/utils/jsonapi.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(o, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mKeyword\u001b[0m \u001b[0marguments\u001b[0m \u001b[0mare\u001b[0m \u001b[0mpassed\u001b[0m \u001b[0malong\u001b[0m \u001b[0mto\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mpy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \"\"\"\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/json/__init__.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0mcheck_circular\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_circular\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_nan\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_nan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0mseparators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseparators\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m         **kw).encode(obj)\n\u001b[0m\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/json/encoder.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;31m# exceptions aren't as detailed.  The list call should be roughly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;31m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_one_shot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/json/encoder.py\u001b[0m in \u001b[0;36miterencode\u001b[0;34m(self, o, _one_shot)\u001b[0m\n\u001b[1;32m    255\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey_separator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem_separator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m                 self.skipkeys, _one_shot)\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m def _make_iterencode(markers, _default, _encoder, _indent, _floatstr,\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}