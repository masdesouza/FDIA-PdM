{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "regression_SpeedingUpCNNs.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNX2zwpsnl+0NgxIHybuJND",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/masdesouza/FDIA-PdM/blob/master/notebooks/regression_SpeedingUpCNNs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5W-lCtST--u"
      },
      "source": [
        "#1. Import modules\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQP_7M4ghdY3"
      },
      "source": [
        "# Regression models: How many more cycles an in-service engine will last before it fails?\n",
        "import tensorflow.keras \n",
        "from tensorflow.keras.layers import Conv1D, BatchNormalization, Dropout, Dense, InputLayer, Flatten, MaxPool1D, Activation, GlobalAveragePooling1D, Conv2D,Conv3D,Lambda\n",
        "from tensorflow.keras.activations import relu, softmax\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.losses import binary_crossentropy\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.models import Sequential,load_model\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from sklearn import preprocessing"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYGmDLpGUqhv"
      },
      "source": [
        "#2. Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vI-RU6t2T-CY"
      },
      "source": [
        "# Setting seed for reproducibility\n",
        "np.random.seed(1234)  \n",
        "PYTHONHASHSEED = 0\n",
        "\n",
        "# define path to save model\n",
        "model_path = '/content/drive/MyDrive/PFC/FDIA-PdM/Trained models/CNN/regression_model_CNN.h5'\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raVt4XEMU-Rt"
      },
      "source": [
        "##2.1  Import dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMhX0JNMUvo3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "133de4e3-e51e-49cb-8a9f-fd56efdbf703"
      },
      "source": [
        "import os\n",
        "import pickle\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "os.chdir(\"/content/drive/MyDrive\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioIgu1t1VMDa"
      },
      "source": [
        "#3. Data Ingestion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iIBaVJpU9Fd"
      },
      "source": [
        "# read training data - It is the aircraft engine run-to-failure data.\n",
        "train_df = pd.read_csv('/content/drive/MyDrive/PFC/FDIA-PdM/Datasets/CMAPSSData/train_FD002.txt', sep=\" \", header=None)\n",
        "train_df.drop(train_df.columns[[26, 27]], axis=1, inplace=True)\n",
        "train_df.columns = ['id', 'cycle', 'setting1', 'setting2', 'setting3', 's1', 's2', 's3',\n",
        "                     's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14',\n",
        "                     's15', 's16', 's17', 's18', 's19', 's20', 's21']\n",
        "\n",
        "train_df = train_df.sort_values(['id','cycle'])\n",
        "\n",
        "# read test data - It is the aircraft engine operating data without failure events recorded.\n",
        "test_df = pd.read_csv('//content/drive/MyDrive/PFC/FDIA-PdM/Datasets/CMAPSSData/test_FD002.txt', sep=\" \", header=None)\n",
        "test_df.drop(test_df.columns[[26, 27]], axis=1, inplace=True)\n",
        "test_df.columns = ['id', 'cycle', 'setting1', 'setting2', 'setting3', 's1', 's2', 's3',\n",
        "                     's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14',\n",
        "                     's15', 's16', 's17', 's18', 's19', 's20', 's21']\n",
        "\n",
        "# read ground truth data - It contains the information of true remaining cycles for each engine in the testing data.\n",
        "truth_df = pd.read_csv('/content/drive/MyDrive/PFC/FDIA-PdM/Datasets/CMAPSSData/RUL_FD002.txt', sep=\" \", header=None)\n",
        "truth_df.drop(truth_df.columns[[1]], axis=1, inplace=True)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Mb6fWj-byi5"
      },
      "source": [
        "#4. Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPL62MSxfxU0"
      },
      "source": [
        "## 4.1 Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59EtRS6rb2R_"
      },
      "source": [
        "# Data Labeling - generate column RUL(Remaining Usefull Life or Time to Failure)\n",
        "rul = pd.DataFrame(train_df.groupby('id')['cycle'].max()).reset_index()\n",
        "rul.columns = ['id', 'max']\n",
        "train_df = train_df.merge(rul, on=['id'], how='left')\n",
        "train_df['RUL'] = train_df['max'] - train_df['cycle']\n",
        "train_df.drop('max', axis=1, inplace=True)\n",
        "\n",
        "# generate label columns for training data\n",
        "# we will only make use of \"label1\" for binary classification, \n",
        "# while trying to answer the question: is a specific engine going to fail within w1 cycles?\n",
        "w1 = 30\n",
        "w0 = 15\n",
        "train_df['label1'] = np.where(train_df['RUL'] <= w1, 1, 0 )\n",
        "train_df['label2'] = train_df['label1']\n",
        "train_df.loc[train_df['RUL'] <= w0, 'label2'] = 2\n",
        "\n",
        "# MinMax normalization (from 0 to 1)\n",
        "train_df['cycle_norm'] = train_df['cycle']\n",
        "cols_normalize = train_df.columns.difference(['id','cycle','RUL','label1','label2'])\n",
        "min_max_scaler = preprocessing.MinMaxScaler()\n",
        "norm_train_df = pd.DataFrame(min_max_scaler.fit_transform(train_df[cols_normalize]), \n",
        "                             columns=cols_normalize, \n",
        "                             index=train_df.index)\n",
        "join_df = train_df[train_df.columns.difference(cols_normalize)].join(norm_train_df)\n",
        "train_df = join_df.reindex(columns = train_df.columns)\n",
        "\n",
        "#train_df.to_csv('../../Dataset/PredictiveTraining.csv', encoding='utf-8',index = None)\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2BELKRKgODN"
      },
      "source": [
        "## 4.2 Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZ_wwC_Uf4pT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa251c23-bbb9-4b1c-a5f4-2fdf99dd38a0"
      },
      "source": [
        "# MinMax normalization (from 0 to 1)\n",
        "test_df['cycle_norm'] = test_df['cycle']\n",
        "norm_test_df = pd.DataFrame(min_max_scaler.transform(test_df[cols_normalize]), \n",
        "                            columns=cols_normalize, \n",
        "                            index=test_df.index)\n",
        "test_join_df = test_df[test_df.columns.difference(cols_normalize)].join(norm_test_df)\n",
        "test_df = test_join_df.reindex(columns = test_df.columns)\n",
        "test_df = test_df.reset_index(drop=True)\n",
        "print(test_df.head())\n",
        "\n",
        "# We use the ground truth dataset to generate labels for the test data.\n",
        "# generate column max for test data\n",
        "rul = pd.DataFrame(test_df.groupby('id')['cycle'].max()).reset_index()\n",
        "rul.columns = ['id', 'max']\n",
        "truth_df.columns = ['more']\n",
        "truth_df['id'] = truth_df.index + 1\n",
        "truth_df['max'] = rul['max'] + truth_df['more']\n",
        "truth_df.drop('more', axis=1, inplace=True)\n",
        "\n",
        "# generate RUL for test data\n",
        "test_df = test_df.merge(truth_df, on=['id'], how='left')\n",
        "test_df['RUL'] = test_df['max'] - test_df['cycle']\n",
        "test_df.drop('max', axis=1, inplace=True)\n",
        "\n",
        "# generate label columns w0 and w1 for test data\n",
        "test_df['label1'] = np.where(test_df['RUL'] <= w1, 1, 0 )\n",
        "test_df['label2'] = test_df['label1']\n",
        "test_df.loc[test_df['RUL'] <= w0, 'label2'] = 2\n",
        "\n",
        "#test_df.to_csv('../../Dataset/PredictiveManteinanceTest.csv', encoding='utf-8',index = None)\n",
        "\n",
        "# pick a large window size of 50 cycles\n",
        "sequence_length = 100\n",
        "\n",
        "# function to reshape features into (samples, time steps, features) \n",
        "def gen_sequence(id_df, seq_length, seq_cols):\n",
        "\n",
        "    data_matrix = id_df[seq_cols].values\n",
        "    num_elements = data_matrix.shape[0]\n",
        "\n",
        "    for start, stop in zip(range(0, num_elements-seq_length), range(seq_length, num_elements)):\n",
        "        yield data_matrix[start:stop, :]\n",
        "\n",
        "# pick the feature columns\n",
        "sensor_cols = ['s' + str(i) for i in range(1,22)]\n",
        "sequence_cols = ['setting1', 'setting2', 'setting3', 'cycle_norm']\n",
        "sequence_cols.extend(sensor_cols)\n",
        "\n",
        "# TODO for debug \n",
        "# val is a list of 192 - 50 = 142 bi-dimensional array (50 rows x 25 columns)\n",
        "val=list(gen_sequence(train_df[train_df['id']==1], sequence_length, sequence_cols))\n",
        "print(len(val))\n",
        "\n",
        "# generator for the sequences\n",
        "# transform each id of the train dataset in a sequence\n",
        "seq_gen = (list(gen_sequence(train_df[train_df['id']==id], sequence_length, sequence_cols)) \n",
        "           for id in train_df['id'].unique())\n",
        "\n",
        "# generate sequences and convert to numpy array\n",
        "seq_array = np.concatenate(list(seq_gen)).astype(np.float32)\n",
        "print(seq_array.shape)\n",
        "\n",
        "# function to generate labels\n",
        "def gen_labels(id_df, seq_length, label):\n",
        "\n",
        "    data_matrix = id_df[label].values\n",
        "    num_elements = data_matrix.shape[0]\n",
        "\n",
        "    return data_matrix[seq_length:num_elements, :]\n",
        "\n",
        "# generate labels\n",
        "label_gen = [gen_labels(train_df[train_df['id']==id], sequence_length, ['RUL']) \n",
        "             for id in train_df['id'].unique()]\n",
        "\n",
        "label_array = np.concatenate(label_gen).astype(np.float32)\n",
        "label_array.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   id  cycle  setting1  setting2  ...  s19       s20       s21  cycle_norm\n",
            "0   1      1  0.238019  0.297150  ...  1.0  0.625514  0.633951    0.000000\n",
            "1   1      2  0.476162  0.831354  ...  1.0  0.483882  0.500205    0.002653\n",
            "2   1      3  0.833282  0.997625  ...  1.0  0.164609  0.165078    0.005305\n",
            "3   1      4  0.999967  0.998812  ...  1.0  0.005830  0.023186    0.007958\n",
            "4   1      5  0.595089  0.737886  ...  0.0  0.141632  0.145822    0.010610\n",
            "\n",
            "[5 rows x 27 columns]\n",
            "49\n",
            "(27759, 100, 25)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(27759, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BICYNX1UgfvP"
      },
      "source": [
        "# 5. Modeling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGMkFQvHNhkB"
      },
      "source": [
        "ExpandDimension = lambda axis: Lambda(lambda x: K.expand_dims(x, axis))\n",
        "SqueezeDimension = lambda axis: Lambda(lambda x: K.squeeze(x, axis))\n",
        "\n",
        "# Layers\n",
        "def simple_factorized_conv(filters, kernel, *args, **kwargs):\n",
        "    kwargs[\"activation\"] = None\n",
        "    def __inner(inp):\n",
        "        cnn1 = Conv2D(filters, (kernel[0], 1), *args, **kwargs)(inp)\n",
        "        cnn2 = Conv2D(filters, (1, kernel[1]), *args, **kwargs)(cnn1)\n",
        "        \n",
        "        return cnn2\n",
        "\n",
        "    return __inner\n",
        "  \n",
        "def cp_decomposed_conv(filters, kernel, *args, **kwargs):\n",
        "    \"\"\"\n",
        "    Beware, it doesn't work! It's just an aproximate demostration of how\n",
        "    CP decomposition should be implemented. The issue is in the dimension matching.\n",
        "    If you found the solution, feel free to comment it, either in the blogpost or here.\n",
        "    You'll get full credit for your finding.\n",
        "    \"\"\"\n",
        "    kwargs[\"activation\"] = None\n",
        "    rank = filters // 2\n",
        "    d = kernel[0]\n",
        "    def __inner(inp):\n",
        "        first    = Conv2D(rank, kernel_size=(1, 1), **kwargs)(inp)\n",
        "        \n",
        "        expanded = ExpandDimension(axis=1)(first)\n",
        "        mid1     = Conv3D(rank, kernel_size=(d, 1, 1), **kwargs)(expanded)\n",
        "        mid2     = Conv3D(rank, kernel_size=(1, d, 1), **kwargs)(mid1)\n",
        "        squeezed = SqueezeDimension(axis=1)(mid2)\n",
        "        \n",
        "        last     = Conv2D(filters,  kernel_size=(1, 1), **kwargs)(squeezed)\n",
        "        \n",
        "        return last\n",
        "\n",
        "    return __inner\n",
        "\n",
        "def bn_relu(layer):\n",
        "    def __inner(*args, **kwargs):\n",
        "        l = layer(*args, **kwargs)\n",
        "        bn = BatchNormalization()(l)\n",
        "        act = Activation(\"relu\")(bn)\n",
        "        \n",
        "        return act # courtesy to Jiun-Kuei Jung\n",
        "\n",
        "    return __inner"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLfYRA3-NpRE"
      },
      "source": [
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import (\n",
        "    Dense, Dropout, Conv2D, SeparableConv2D,\n",
        "    MaxPool2D, Flatten, GlobalAveragePooling2D\n",
        ")\n",
        "\n",
        "class BaseExampleModel:\n",
        "    def __init__(self, conv_type=\"std\"):\n",
        "        \"\"\"\n",
        "        :convtype: used as a flag to easily switch the Convolution layer implementation\n",
        "        \"\"\"\n",
        "        self.conv_layer = lambda *args, **kwargs: {\n",
        "                              \"std\": bn_relu(Conv2D(*args, **kwargs)),\n",
        "                              \"sep\": bn_relu(SeparableConv2D(*args, **kwargs)),\n",
        "                              \"cp\" : bn_relu(cp_decomposed_conv(*args, **kwargs)),\n",
        "                              \"fac\": bn_relu(simple_factorized_conv(*args, **kwargs))\n",
        "                          }[conv_type]\n",
        "        \n",
        "    def make(self, inp, nb_classes):\n",
        "        \"\"\"\n",
        "        :inp: a reference to the Keras Input layer, to easily switch the input size\n",
        "              and even the way data is fed into network\n",
        "              (via standard Keras methods or via TF Dataset API)\n",
        "        :nb_classes: for the final Dense layer\n",
        "        \"\"\"\n",
        "        NotImplemented\n",
        "\n",
        "class AllCNNLike(BaseExampleModel):\n",
        "    \"\"\"\n",
        "    isnpired from https://arxiv.org/abs/1412.6806\n",
        "    namely All-CNN-C\n",
        "    \"\"\"\n",
        "    def __init__(self, conv_type=\"std\"):\n",
        "        super().__init__(conv_type=conv_type)\n",
        "        \n",
        "    def make(self, inp, nb_classes):\n",
        "        conv1 = self.conv_layer(96, (3, 3), padding=\"same\")(inp)\n",
        "        conv2 = self.conv_layer(96, (3, 3), padding=\"same\")(conv1)\n",
        "        \n",
        "        conv3 = bn_relu(Conv2D(96, (3, 3), padding=\"same\", strides=(2, 2)))(conv2)\n",
        "        \n",
        "        conv4 = self.conv_layer(192, (3, 3), padding=\"same\")(conv3)\n",
        "        conv5 = self.conv_layer(192, (3, 3), padding=\"same\")(conv4)\n",
        "        \n",
        "        conv6 = bn_relu(Conv2D(192, (3, 3), padding=\"same\", strides=(2, 2)))(conv5)\n",
        "        \n",
        "        conv7 = bn_relu(Conv2D(192, (3, 3), padding=\"same\"))(conv6)\n",
        "        conv8 = bn_relu(Conv2D(192, (1, 1), padding=\"same\"))(conv7)\n",
        "        conv9 = bn_relu(Conv2D(nb_classes, (1, 1), padding=\"same\"))(conv8)\n",
        "        \n",
        "        gap = GlobalAveragePooling2D()(conv9)\n",
        "        final = Activation(\"softmax\")(gap)\n",
        "        \n",
        "        return Model(inputs=inp, outputs=final)\n",
        "      \n",
        "class WiderAllCNNLike(BaseExampleModel):\n",
        "    \"\"\"\n",
        "    isnpired from https://arxiv.org/abs/1412.6806\n",
        "    namely All-CNN-C\n",
        "    \"\"\"\n",
        "    def __init__(self, conv_type=\"std\"):\n",
        "        super().__init__(conv_type=conv_type)\n",
        "        \n",
        "    def make(self, inp, nb_classes):\n",
        "        conv1 = self.conv_layer(192, (3, 3), padding=\"same\")(inp)\n",
        "        \n",
        "        conv2 = bn_relu(Conv2D(96, (3, 3), padding=\"same\", strides=(2, 2)))(conv1)\n",
        "        \n",
        "        conv3 = self.conv_layer(384, (3, 3), padding=\"same\")(conv2)\n",
        "        \n",
        "        conv4 = bn_relu(Conv2D(128, (3, 3), padding=\"same\", strides=(2, 2)))(conv3)\n",
        "        \n",
        "        conv5 = bn_relu(Conv2D(256, (3, 3), padding=\"same\"))(conv4)\n",
        "        conv6 = bn_relu(Conv2D(nb_classes, (1, 1), padding=\"same\"))(conv5)\n",
        "        \n",
        "        gap = GlobalAveragePooling2D()(conv6)\n",
        "        final = Activation(\"softmax\")(gap)\n",
        "        \n",
        "        return Model(inputs=inp, outputs=final)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wHAMJ1-NyyF"
      },
      "source": [
        "from tensorflow.keras.metrics import categorical_accuracy, top_k_categorical_accuracy\n",
        "from tensorflow.keras.optimizers import  SGD\n",
        "from tensorflow.keras.layers import Input\n",
        "\n",
        "def main(conv_type=\"std\", verbose=False, net_type=\"simple\"):\n",
        "    inp = Input(shape=(32, 32, 3))\n",
        "    net = {\n",
        "        \"simple\": AllCNNLike(conv_type=conv_type).make(inp, nb_classes=100),\n",
        "        \"wide\": WiderAllCNNLike(conv_type=conv_type).make(inp, nb_classes=100)\n",
        "    }[net_type]\n",
        "\n",
        "    net.compile(SGD(lr=0.01, decay=0.001, momentum=0.8), \"categorical_crossentropy\",\n",
        "                 metrics=[categorical_accuracy, top_k_categorical_accuracy])\n",
        "\n",
        "    if verbose:\n",
        "        print(net.summary())\n",
        "\n",
        "    net.fit_generator(train_df, steps_per_epoch=50000 // BATCH_SIZE, epochs=5, verbose=verbose)\n",
        "    loss, acc, topk_acc = net.evaluate_generator(test_df, steps=10000 // BATCH_SIZE)\n",
        "\n",
        "    print(f\"Accuracy: {acc}, Top5 Accuracy {topk_acc} and Loss {loss}.\")"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3FLziWkgNh5"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def r2_keras(y_true, y_pred):\n",
        "    \"\"\"Coefficient of Determination \n",
        "    \"\"\"\n",
        "    SS_res =  K.sum(K.square( y_true - y_pred ))\n",
        "    SS_tot = K.sum(K.square( y_true - K.mean(y_true) ) )\n",
        "    return ( 1 - SS_res/(SS_tot + K.epsilon()) )\n",
        "\n",
        "def rmse(y_true, y_pred):\n",
        "    return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1))\n",
        "# Next, we build a deep network. \n",
        "# The first layer is an LSTM layer with 100 units followed by another LSTM layer with 50 units. \n",
        "\n",
        "nb_features = seq_array.shape[2]\n",
        "nb_out = label_array.shape[1]\n",
        "\n",
        "cnn = Sequential()\n",
        "cnn.add(InputLayer(input_shape=(sequence_length, nb_features)))\n",
        "cnn.add(BatchNormalization(axis=-1))  #Scaling the data\n",
        "\n",
        "cnn.add(Conv1D(filters=64,\n",
        "               kernel_size=3,\n",
        "               padding=\"valid\",\n",
        "               activation=\"relu\",\n",
        "               kernel_regularizer='l2'\n",
        "               )\n",
        "       )\n",
        "# cnn.add(MaxPool1D(pool_size=2))\n",
        "# cnn.add(BatchNormalization(axis=-1))\n",
        "cnn.add(Conv1D(filters=64,\n",
        "               kernel_size=3,\n",
        "               padding=\"valid\",\n",
        "               activation=\"relu\",\n",
        "               kernel_regularizer='l2')\n",
        "       )\n",
        "\n",
        "cnn.add(Conv1D(filters=64,\n",
        "               kernel_size=3,\n",
        "               padding=\"valid\",\n",
        "               activation=\"relu\",\n",
        "               kernel_regularizer='l2')\n",
        "       )\n",
        "# cnn.add(BatchNormalization(axis=-1))\n",
        "# cnn.add(MaxPool1D(pool_size=2))\n",
        "# cnn.add(BatchNormalization(axis=-1))\n",
        "\n",
        "cnn.add(Conv1D(filters=64,\n",
        "               kernel_size=3,\n",
        "               padding=\"valid\",\n",
        "               activation=\"relu\",\n",
        "               kernel_regularizer='l2')\n",
        "       )\n",
        "\n",
        "cnn.add(Flatten())\n",
        "cnn.add(Dense(40))\n",
        "cnn.add(Activation('relu'))\n",
        "cnn.add(Dense(30))\n",
        "cnn.add(Activation('relu'))\n",
        "cnn.add(Dense(units=nb_out))\n",
        "cnn.add(Activation(\"relu\"))\n",
        "# cnn.add(Dense(30))\n",
        "# cnn.add(Activation('relu'))\n",
        "# cnn.add(Dense(20))\n",
        "# cnn.add(Activation('relu'))\n",
        "# cnn.add(Dense(10))\n",
        "# cnn.add(Activation('relu'))\n",
        "# cnn.add(Dense(units=nb_out))\n",
        "# cnn.add(Activation('relu'))\n",
        "cnn.compile(loss='mean_squared_error', optimizer='rmsprop',metrics=[rmse,r2_keras])\n",
        "\n",
        "print(cnn.summary())\n",
        "\n",
        "history = cnn.fit(seq_array, label_array, epochs=100, batch_size=512, validation_split=0.05, verbose=2,\n",
        "          callbacks = [tensorflow.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=0, mode='min'),\n",
        "                       tensorflow.keras.callbacks.ModelCheckpoint(model_path,monitor='val_loss', save_best_only=True, mode='min', verbose=0)]\n",
        "    )\n",
        "\n",
        "# model = Sequential()\n",
        "# model.add(LSTM(\n",
        "#          input_shape=(sequence_length, nb_features),\n",
        "#          units=100,\n",
        "#          return_sequences=True))\n",
        "# model.add(Dropout(0.2))\n",
        "# model.add(LSTM(\n",
        "#           units=100,\n",
        "#           return_sequences=False))\n",
        "# model.add(Dropout(0.2))\n",
        "# model.add(Dense(units=nb_out))\n",
        "# model.add(Activation(\"linear\"))\n",
        "# model.compile(loss='mean_squared_error', optimizer='rmsprop',metrics=['mae',r2_keras])\n",
        "#\n",
        "# print(model.summary())\n",
        "#\n",
        "#\n",
        "#\n",
        "# # fit the network\n",
        "# history = model.fit(seq_array, label_array, epochs=100, batch_size=200, validation_split=0.05, verbose=2,\n",
        "#           callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=0, mode='min'),\n",
        "#                        keras.callbacks.ModelCheckpoint(model_path,monitor='val_loss', save_best_only=True, mode='min', verbose=0)]\n",
        "#           )\n",
        "\n",
        "# list all data in history\n",
        "print(history.history.keys())\n",
        "\n",
        "\n",
        "# training metrics\n",
        "scores = cnn.evaluate(seq_array, label_array, verbose=1, batch_size=200)\n",
        "print('\\nMAE: {}'.format(scores[1]))\n",
        "print('\\nR^2: {}'.format(scores[2]))\n",
        "\n",
        "y_pred = cnn.predict(seq_array,verbose=1, batch_size=200)\n",
        "y_true = label_array\n",
        "\n",
        "test_set = pd.DataFrame(y_pred)\n",
        "test_set.to_csv('/content/drive/MyDrive/PFC/FDIA-PdM/Output/submit_train_CNN.csv', index = None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-AupfE_hg4J"
      },
      "source": [
        "#6. EVALUATE ON TEST DATA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKlGVtWBhgNx"
      },
      "source": [
        "# We pick the last sequence for each id in the test data\n",
        "seq_array_test_last = [test_df[test_df['id']==id][sequence_cols].values[-sequence_length:] \n",
        "                       for id in test_df['id'].unique() if len(test_df[test_df['id']==id]) >= sequence_length]\n",
        "\n",
        "seq_array_test_last = np.asarray(seq_array_test_last).astype(np.float32)\n",
        "print(\"seq_array_test_last\")\n",
        "#print(seq_array_test_last)\n",
        "print(seq_array_test_last.shape)\n",
        "\n",
        "# Similarly, we pick the labels\n",
        "#print(\"y_mask\")\n",
        "y_mask = [len(test_df[test_df['id']==id]) >= sequence_length for id in test_df['id'].unique()]\n",
        "label_array_test_last = test_df.groupby('id')['RUL'].nth(-1)[y_mask].values\n",
        "label_array_test_last = label_array_test_last.reshape(label_array_test_last.shape[0],1).astype(np.float32)\n",
        "print(label_array_test_last.shape)\n",
        "print(\"label_array_test_last\")\n",
        "print(label_array_test_last)\n",
        "\n",
        "# if best iteration's model was saved then load and use it\n",
        "if os.path.isfile(model_path):\n",
        "    cnn.load_weights(model_path)\n",
        "    #estimator = load_model(model_path,custom_objects={'r2_keras': r2_keras})\n",
        "\n",
        "    # test metrics\n",
        "    scores_test = cnn.evaluate(seq_array_test_last, label_array_test_last, verbose=2)\n",
        "    print('\\nMAE: {}'.format(scores_test[1]))\n",
        "    print('\\nR^2: {}'.format(scores_test[2]))\n",
        "\n",
        "    y_pred_test = cnn.predict(seq_array_test_last)\n",
        "    y_true_test = label_array_test_last\n",
        "    print(\"Prediction\")\n",
        "    print(y_pred_test);\n",
        "    print(\"Truth\")\n",
        "    print(y_true_test);\n",
        "\n",
        "    # Plot in blue color the predicted data and in green color the\n",
        "    # actual data to verify visually the accuracy of the model.\n",
        "    fig_verify = plt.figure(figsize=(100, 50))\n",
        "    plt.plot(y_pred_test, color=\"blue\")\n",
        "    plt.plot(y_true_test, color=\"green\")\n",
        "    plt.title('prediction')\n",
        "    plt.ylabel('RUL in cycles')\n",
        "    plt.xlabel('Engine ID')\n",
        "    plt.legend(['predicted', 'actual data'], loc='upper left')\n",
        "    plt.show()\n",
        "    fig_verify.savefig(\"/content/drive/MyDrive/PFC/FDIA-PdM/Output/model_regression_verify_CNN.png\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}