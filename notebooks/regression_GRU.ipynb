{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "regression_GRU.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMn7Xi60MmWBGQs9aHpY3FN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/masdesouza/FDIA-PdM/blob/master/notebooks/regression_GRU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYjN2bqIgmBg"
      },
      "source": [
        "# 1. Import modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOCXNVQ4gnvD"
      },
      "source": [
        "# Regression models: How many more cycles an in-service engine will last before it fails?\n",
        "import tensorflow.keras\n",
        "import tensorflow.keras.backend as K\n",
        "#from tensorflow.keras.layers.core import Activation\n",
        "from tensorflow.keras.activations import tanh, linear\n",
        "from tensorflow.keras.models import Sequential,load_model\n",
        "from tensorflow.keras.layers import Dense, Dropout, GRU\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from sklearn import preprocessing"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2b4jfSsgoXS"
      },
      "source": [
        "#2. Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y41vFJl-gsTf"
      },
      "source": [
        "# Setting seed for reproducibility\n",
        "np.random.seed(1234)  \n",
        "PYTHONHASHSEED = 0\n",
        "\n",
        "# define path to save model\n",
        "model_path = '/content/drive/MyDrive/PFC/FDIA-PdM/Trained models/GRU/regression_model_GRU.h5'"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jsx-SQ88gtGK"
      },
      "source": [
        "##2.1 Import dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2zwK5inlgtzv",
        "outputId": "d3fe449c-6c1f-4625-9d11-8e4db5308023"
      },
      "source": [
        "import os\n",
        "import pickle\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "os.chdir(\"/content/drive/MyDrive\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fH0DPOUsg1wM"
      },
      "source": [
        "# 3. Data Ingestion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8CunJNeXg6xh"
      },
      "source": [
        "# read training data - It is the aircraft engine run-to-failure data.\n",
        "train_df = pd.read_csv('/content/drive/MyDrive/PFC/FDIA-PdM/Datasets/CMAPSSData/train_FD002.txt', sep=\" \", header=None)\n",
        "train_df.drop(train_df.columns[[26, 27]], axis=1, inplace=True)\n",
        "train_df.columns = ['id', 'cycle', 'setting1', 'setting2', 'setting3', 's1', 's2', 's3',\n",
        "                     's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14',\n",
        "                     's15', 's16', 's17', 's18', 's19', 's20', 's21']\n",
        "\n",
        "train_df = train_df.sort_values(['id','cycle'])\n",
        "\n",
        "# read test data - It is the aircraft engine operating data without failure events recorded.\n",
        "test_df = pd.read_csv('/content/drive/MyDrive/PFC/FDIA-PdM/Datasets/CMAPSSData/test_FD002.txt', sep=\" \", header=None)\n",
        "test_df.drop(test_df.columns[[26, 27]], axis=1, inplace=True)\n",
        "test_df.columns = ['id', 'cycle', 'setting1', 'setting2', 'setting3', 's1', 's2', 's3',\n",
        "                     's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14',\n",
        "                     's15', 's16', 's17', 's18', 's19', 's20', 's21']\n",
        "\n",
        "# read ground truth data - It contains the information of true remaining cycles for each engine in the testing data.\n",
        "truth_df = pd.read_csv('/content/drive/MyDrive/PFC/FDIA-PdM/Datasets/CMAPSSData/RUL_FD002.txt', sep=\" \", header=None)\n",
        "truth_df.drop(truth_df.columns[[1]], axis=1, inplace=True)\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOTueI_xg8xO"
      },
      "source": [
        "#4. Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-GH1JtLhABg"
      },
      "source": [
        "## 4.1 Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gG25eJqXhGZ6"
      },
      "source": [
        "# Data Labeling - generate column RUL(Remaining Usefull Life or Time to Failure)\n",
        "rul = pd.DataFrame(train_df.groupby('id')['cycle'].max()).reset_index()\n",
        "rul.columns = ['id', 'max']\n",
        "train_df = train_df.merge(rul, on=['id'], how='left')\n",
        "train_df['RUL'] = train_df['max'] - train_df['cycle']\n",
        "train_df.drop('max', axis=1, inplace=True)\n",
        "\n",
        "# generate label columns for training data\n",
        "# we will only make use of \"label1\" for binary classification, \n",
        "# while trying to answer the question: is a specific engine going to fail within w1 cycles?\n",
        "w1 = 30\n",
        "w0 = 15\n",
        "train_df['label1'] = np.where(train_df['RUL'] <= w1, 1, 0 )\n",
        "train_df['label2'] = train_df['label1']\n",
        "train_df.loc[train_df['RUL'] <= w0, 'label2'] = 2\n",
        "\n",
        "# MinMax normalization (from 0 to 1)\n",
        "train_df['cycle_norm'] = train_df['cycle']\n",
        "cols_normalize = train_df.columns.difference(['id','cycle','RUL','label1','label2'])\n",
        "min_max_scaler = preprocessing.MinMaxScaler()\n",
        "norm_train_df = pd.DataFrame(min_max_scaler.fit_transform(train_df[cols_normalize]), \n",
        "                             columns=cols_normalize, \n",
        "                             index=train_df.index)\n",
        "join_df = train_df[train_df.columns.difference(cols_normalize)].join(norm_train_df)\n",
        "train_df = join_df.reindex(columns = train_df.columns)\n",
        "\n",
        "#train_df.to_csv('../../Dataset/PredictiveTraining.csv', encoding='utf-8',index = None)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ok4qJFs6hGsl"
      },
      "source": [
        "## 4.2 Test\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aSpgcRqKhKZV",
        "outputId": "710e0092-5c8d-4096-f9fd-95685ac15ea5"
      },
      "source": [
        "# MinMax normalization (from 0 to 1)\n",
        "test_df['cycle_norm'] = test_df['cycle']\n",
        "norm_test_df = pd.DataFrame(min_max_scaler.transform(test_df[cols_normalize]), \n",
        "                            columns=cols_normalize, \n",
        "                            index=test_df.index)\n",
        "test_join_df = test_df[test_df.columns.difference(cols_normalize)].join(norm_test_df)\n",
        "test_df = test_join_df.reindex(columns = test_df.columns)\n",
        "test_df = test_df.reset_index(drop=True)\n",
        "print(test_df.head())\n",
        "\n",
        "# We use the ground truth dataset to generate labels for the test data.\n",
        "# generate column max for test data\n",
        "rul = pd.DataFrame(test_df.groupby('id')['cycle'].max()).reset_index()\n",
        "rul.columns = ['id', 'max']\n",
        "truth_df.columns = ['more']\n",
        "truth_df['id'] = truth_df.index + 1\n",
        "truth_df['max'] = rul['max'] + truth_df['more']\n",
        "truth_df.drop('more', axis=1, inplace=True)\n",
        "\n",
        "# generate RUL for test data\n",
        "test_df = test_df.merge(truth_df, on=['id'], how='left')\n",
        "test_df['RUL'] = test_df['max'] - test_df['cycle']\n",
        "test_df.drop('max', axis=1, inplace=True)\n",
        "\n",
        "# generate label columns w0 and w1 for test data\n",
        "test_df['label1'] = np.where(test_df['RUL'] <= w1, 1, 0 )\n",
        "test_df['label2'] = test_df['label1']\n",
        "test_df.loc[test_df['RUL'] <= w0, 'label2'] = 2\n",
        "\n",
        "#test_df.to_csv('../../Dataset/PredictiveManteinanceTest.csv', encoding='utf-8',index = None)\n",
        "\n",
        "# pick a large window size of 50 cycles\n",
        "sequence_length = 80\n",
        "\n",
        "# function to reshape features into (samples, time steps, features) \n",
        "def gen_sequence(id_df, seq_length, seq_cols):\n",
        "\n",
        "    data_matrix = id_df[seq_cols].values\n",
        "    num_elements = data_matrix.shape[0]\n",
        "\n",
        "    for start, stop in zip(range(0, num_elements-seq_length), range(seq_length, num_elements)):\n",
        "        yield data_matrix[start:stop, :]\n",
        "\n",
        "# pick the feature columns\n",
        "sensor_cols = ['s' + str(i) for i in range(1,22)]\n",
        "sequence_cols = ['setting1', 'setting2', 'setting3', 'cycle_norm']\n",
        "sequence_cols.extend(sensor_cols)\n",
        "\n",
        "# TODO for debug \n",
        "# val is a list of 192 - 50 = 142 bi-dimensional array (50 rows x 25 columns)\n",
        "val=list(gen_sequence(train_df[train_df['id']==1], sequence_length, sequence_cols))\n",
        "print(len(val))\n",
        "\n",
        "# generator for the sequences\n",
        "# transform each id of the train dataset in a sequence\n",
        "seq_gen = (list(gen_sequence(train_df[train_df['id']==id], sequence_length, sequence_cols)) \n",
        "           for id in train_df['id'].unique())\n",
        "\n",
        "# generate sequences and convert to numpy array\n",
        "seq_array = np.concatenate(list(seq_gen)).astype(np.float32)\n",
        "print(seq_array.shape)\n",
        "\n",
        "# function to generate labels\n",
        "def gen_labels(id_df, seq_length, label):\n",
        "\n",
        "    data_matrix = id_df[label].values\n",
        "    num_elements = data_matrix.shape[0]\n",
        "\n",
        "    return data_matrix[seq_length:num_elements, :]\n",
        "\n",
        "# generate labels\n",
        "label_gen = [gen_labels(train_df[train_df['id']==id], sequence_length, ['RUL']) \n",
        "             for id in train_df['id'].unique()]\n",
        "\n",
        "label_array = np.concatenate(label_gen).astype(np.float32)\n",
        "label_array.shape"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   id  cycle  setting1  setting2  ...  s19       s20       s21  cycle_norm\n",
            "0   1      1  0.238019  0.297150  ...  1.0  0.625514  0.633951    0.000000\n",
            "1   1      2  0.476162  0.831354  ...  1.0  0.483882  0.500205    0.002653\n",
            "2   1      3  0.833282  0.997625  ...  1.0  0.164609  0.165078    0.005305\n",
            "3   1      4  0.999967  0.998812  ...  1.0  0.005830  0.023186    0.007958\n",
            "4   1      5  0.595089  0.737886  ...  0.0  0.141632  0.145822    0.010610\n",
            "\n",
            "[5 rows x 27 columns]\n",
            "69\n",
            "(32959, 80, 25)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(32959, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbLvtnFVhOp2"
      },
      "source": [
        "#5. Modeling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "id": "5KkvgBcthSAp",
        "outputId": "a7864e93-dff5-4cc9-cd57-3bb9bad43f51"
      },
      "source": [
        "def rmse(y_true, y_pred):\n",
        "    return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1))\n",
        "\n",
        "def r2_keras(y_true, y_pred):\n",
        "    \"\"\"Coefficient of Determination \n",
        "    \"\"\"\n",
        "    SS_res =  K.sum(K.square( y_true - y_pred ))\n",
        "    SS_tot = K.sum(K.square( y_true - K.mean(y_true) ) )\n",
        "    return ( 1 - SS_res/(SS_tot + K.epsilon()) )\n",
        "# Next, we build a deep network. \n",
        "# The first layer is an LSTM layer with 100 units followed by another LSTM layer with 50 units. \n",
        "\n",
        "nb_features = seq_array.shape[2]\n",
        "nb_out = label_array.shape[1]\n",
        "\n",
        "model = Sequential()\n",
        "model.add(GRU(input_shape=(sequence_length, nb_features),units=100, activation='tanh', return_sequences=True))\n",
        "model.add(Dropout(0.15))  # Dropout overfitting\n",
        "\n",
        "# model.add(GRU(layers[2],activation='tanh', return_sequences=True))\n",
        "# model.add(Dropout(0.2))  # Dropout overfitting\n",
        "\n",
        "model.add(GRU(100, activation='tanh', return_sequences=True))\n",
        "model.add(Dropout(0.15))  # Dropout overfitting\n",
        "\n",
        "model.add(GRU(100, activation='tanh', return_sequences=True))\n",
        "model.add(Dropout(0.15))  # Dropout overfitting\n",
        "\n",
        "#model.add(GRU(64, activation='tanh', return_sequences=True))\n",
        "#model.add(Dropout(0.15))  # Dropout overfitting\n",
        "\n",
        "model.add(GRU(100, activation='tanh', return_sequences=False))\n",
        "model.add(Dropout(0.15))  # Dropout overfitting\n",
        "\n",
        "\n",
        "model.add(Dense(units=nb_out))\n",
        "model.add(Activation(\"linear\"))\n",
        "model.compile(loss='mean_squared_error', optimizer='rmsprop',metrics=[rmse,r2_keras])\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "# fit the network\n",
        "history = model.fit(seq_array, label_array, epochs=100, batch_size=200, validation_split=0.05, verbose=2,\n",
        "          callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=0, mode='min'),\n",
        "                       keras.callbacks.ModelCheckpoint(model_path,monitor='val_loss', save_best_only=True, mode='min', verbose=0)]\n",
        "          )\n",
        "\n",
        "# list all data in history\n",
        "print(history.history.keys())\n",
        "\n",
        "\n",
        "# training metrics\n",
        "scores = model.evaluate(seq_array, label_array, verbose=1, batch_size=200)\n",
        "print('\\nRMSE: {}'.format(scores[1]))\n",
        "print('\\nR^2: {}'.format(scores[2]))\n",
        "\n",
        "y_pred = model.predict(seq_array,verbose=1, batch_size=200)\n",
        "y_true = label_array\n",
        "\n",
        "test_set = pd.DataFrame(y_pred)\n",
        "test_set.to_csv('/content/drive/MyDrive/PFC/FDIA-PdM/Output/submit_train_GRU.csv', index = None)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-dd0b91c7226d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnb_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mActivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"linear\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mean_squared_error'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rmsprop'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrmse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr2_keras\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Activation' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqbrK2DLhTTZ"
      },
      "source": [
        "#6. EVALUATE ON TEST DATA\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwAnSlN_hWaa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}